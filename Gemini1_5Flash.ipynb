{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPaS5J+o1I1uliDqfs1kb5d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vrangayyan6/GenAI/blob/main/Gemini1_5Flash.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Claude.ai Artifacts\n",
        "\n",
        "Refer to presentation https://docs.google.com/presentation/d/1v7fqNT_BdIpRRa3i7_ebyIPNo-owByvyoYx4Y5O2inM/edit?usp=sharing"
      ],
      "metadata": {
        "id": "zyjagrNAO4xK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Prompt: Create a Google Colab notebook to connect to Google Gemini 1.5 Flash. Use the Claude Artifacts feature.\n",
        "\n",
        "I apologize, but I'm not able to actually create or access a Google Colab notebook. However, I can provide you with a code template that you could use to create such a notebook to connect to Google Gemini 1.5 Flash. Let me prepare that for you.\n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "# Install the required libraries\n",
        "!pip install google-generativeai\n",
        "\n",
        "import google.generativeai as genai\n",
        "import os\n",
        "\n",
        "# Set up your API key\n",
        "# Replace 'YOUR_API_KEY' with your actual Gemini API key\n",
        "os.environ['GOOGLE_API_KEY'] = 'YOUR_API_KEY'\n",
        "genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
        "\n",
        "# Initialize the model\n",
        "model = genai.GenerativeModel('gemini-1.5-pro')\n",
        "\n",
        "# Function to generate content\n",
        "def generate_content(prompt):\n",
        "    response = model.generate_content(prompt)\n",
        "    return response.text\n",
        "\n",
        "def main():\n",
        "    print(\"Welcome to the Gemini 1.5 Flash Demo!\")\n",
        "    print(\"Enter your prompts below. Type 'quit' to exit.\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"\\nEnter your prompt: \")\n",
        "        if user_input.lower() == 'quit':\n",
        "            print(\"Thank you for using the Gemini 1.5 Flash Demo. Goodbye!\")\n",
        "            break\n",
        "\n",
        "        try:\n",
        "            response = generate_response(user_input)\n",
        "            display(Markdown(response))\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "```\n",
        "\n",
        "I've created a code template for a Google Colab notebook that you can use to connect to Google Gemini 1.5 Flash. This template includes:\n",
        "\n",
        "1. Installation of the required library (google-generativeai)\n",
        "2. Setting up the API key (you'll need to replace 'YOUR_API_KEY' with your actual Gemini API key)\n",
        "3. Initializing the model (using 'gemini-1.5-pro' as the model name)\n",
        "4. A function to generate content based on prompts\n",
        "5. An example usage of the model\n",
        "6. An interactive loop for multiple prompts\n",
        "\n",
        "To use this in Google Colab:\n",
        "\n",
        "1. Create a new notebook in Google Colab\n",
        "2. Copy and paste this code into a code cell\n",
        "3. Replace 'YOUR_API_KEY' with your actual Gemini API key\n",
        "4. Run the cell\n",
        "\n",
        "Please note that you'll need to have access to the Gemini 1.5 Flash API for this to work. As of my last update in April 2024, Gemini 1.5 Flash was not widely available, so you may need to check if you have access or if there have been any updates to the API since then.\n",
        "\n",
        "Also, keep in mind that the exact model name ('gemini-1.5-pro' in this code) might need to be adjusted based on the final naming convention used by Google for Gemini 1.5 Flash.\n",
        "\n",
        "Would you like me to explain any part of this code in more detail?\n"
      ],
      "metadata": {
        "id": "tlpLQgPnOTJ4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Qqk0cVYPhYy"
      },
      "outputs": [],
      "source": [
        "# Install the required libraries\n",
        "!pip install -q -U google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# Set up the API key\n",
        "os.environ['GOOGLE_API_KEY'] = 'YOUR_API_KEY'\n",
        "\n",
        "# Configure the generative AI\n",
        "genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
        "\n",
        "# Initialize the model\n",
        "# model = genai.GenerativeModel('gemini-1.5-pro')\n",
        "model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "\n",
        "def generate_response(prompt):\n",
        "    \"\"\"Generate a response using the Gemini 1.5 Flash model.\"\"\"\n",
        "    response = model.generate_content(prompt)\n",
        "    return response.text\n",
        "\n",
        "def main():\n",
        "    print(\"Welcome to the Gemini 1.5 Flash Demo!\")\n",
        "    print(\"Enter your prompts below. Type 'quit' to exit.\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"\\nEnter your prompt: \")\n",
        "        if user_input.lower() == 'quit':\n",
        "            print(\"Thank you for using the Gemini 1.5 Flash Demo. Goodbye!\")\n",
        "            break\n",
        "\n",
        "        try:\n",
        "            response = generate_response(user_input)\n",
        "            display(Markdown(response))\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 712
        },
        "id": "P8PWHciaQjOI",
        "outputId": "25546092-9950-4eb3-bced-2b9f779e711d"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Welcome to the Gemini 1.5 Flash Demo!\n",
            "Enter your prompts below. Type 'quit' to exit.\n",
            "\n",
            "Enter your prompt: Act as an expert road traveler.  Advice on scenic spots  on the road half way between Atlanta GA and Urbana IL.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Halfway Between Atlanta and Urbana: Scenic Stops You Won't Want to Miss\n\nYou're looking for a road trip halfway between Atlanta, Georgia and Urbana, Illinois? That puts you roughly in the heart of the Ozarks, which means you're in for a treat!  Here's a breakdown of scenic spots to consider, depending on your interests:\n\n**For the Nature Lover:**\n\n* **Ozark National Scenic Riverways (Missouri):** This park offers stunning Ozark scenery, perfect for hiking, canoeing, or simply enjoying the tranquility.  \n    * **Highlight:** Float the Current River for an unforgettable experience.\n* **Elephant Rocks State Park (Missouri):**  A unique rock formation that resembles elephants, perfect for climbing, picnicking, and exploring.\n* **Mark Twain National Forest (Missouri):** A vast forest teeming with hiking trails, waterfalls, and wildlife.\n    * **Highlight:** Visit the stunning Alley Spring for a refreshing swim.\n* **The Shawnee National Forest (Illinois):**  A hidden gem with incredible views,  perfect for hiking, horseback riding, or simply enjoying the natural beauty.\n    * **Highlight:** Hike to the Garden of the Gods, a rock formation with breathtaking views.\n\n**For the History Buff:**\n\n* **Missouri Wine Country:**  Discover wineries with charming tasting rooms and beautiful vineyard views.\n* **The Gateway Arch (St. Louis, Missouri):**  A must-see landmark that symbolizes the westward expansion of the United States.\n* **Cahokia Mounds State Historic Site (Illinois):**  Explore the remnants of a prehistoric Native American city, offering a glimpse into ancient civilizations.\n* **Route 66:**  Take a nostalgic drive along the iconic \"Mother Road\", stopping at vintage diners and roadside attractions.\n\n**For the Foodie:**\n\n* **St. Louis:**  Explore the city's vibrant food scene, from authentic barbecue to world-class restaurants.\n* **Springfield, Missouri:**  Home to the famous \"Route 66\" diner culture and delicious southern comfort food.\n* **The Ozarks:**  Sample local specialties like Ozark trout, Ozark fudge, and handcrafted brews.\n\n**Pro Tips for Your Scenic Road Trip:**\n\n* **Time of Year:** Spring and fall offer vibrant foliage, while summer brings warm weather and outdoor activities.\n* **Plan Ahead:** Research and book accommodations in advance, especially during peak season.\n* **Pack for the Unexpected:** Bring layers of clothing, sunscreen, insect repellent, and a first-aid kit.\n* **Embrace the Journey:** Slow down, enjoy the scenery, and take the time to explore. \n\nThis is just a starting point, and there are endless possibilities for scenic stops along the way.  Happy travels! \n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Enter your prompt: quit\n",
            "Thank you for using the Gemini 1.5 Flash Demo. Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Prompt: Please add code for grounding in Google search results and providing citations.\n",
        "\n",
        "###Response:\n",
        "Certainly, I can help you add functionality for grounding responses in Google search results and providing citations. However, it's important to note that as of my last update in April 2024, Google didn't provide a direct API for search results that could be easily integrated with Gemini. Instead, we'll use a third-party library called `googlesearch-python` to fetch search results, and then use these results to ground Gemini's responses.\n",
        "\n",
        "Let's update our code to include this functionality:\n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "# Install the required libraries\n",
        "!pip install google-generativeai googlesearch-python\n",
        "\n",
        "import google.generativeai as genai\n",
        "import os\n",
        "from googlesearch import search\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Set up your API key\n",
        "# Replace 'YOUR_API_KEY' with your actual Gemini API key\n",
        "os.environ['GOOGLE_API_KEY'] = 'YOUR_API_KEY'\n",
        "genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
        "\n",
        "# Initialize the model\n",
        "model = genai.GenerativeModel('gemini-1.5-pro')\n",
        "\n",
        "def get_search_results(query, num_results=5):\n",
        "    results = []\n",
        "    for j in search(query, num_results=num_results):\n",
        "        results.append(j)\n",
        "    return results\n",
        "\n",
        "def get_webpage_content(url):\n",
        "    try:\n",
        "        response = requests.get(url, timeout=5)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        return soup.get_text()[:1000]  # Get first 1000 characters\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "def generate_grounded_content(prompt):\n",
        "    # Get search results\n",
        "    search_results = get_search_results(prompt)\n",
        "    \n",
        "    # Fetch content from search results\n",
        "    search_contents = [f\"Source {i+1}: {get_webpage_content(url)}\" for i, url in enumerate(search_results)]\n",
        "    \n",
        "    # Combine prompt with search contents\n",
        "    grounded_prompt = f\"\"\"\n",
        "    Based on the following information, please answer the question or respond to the prompt:\n",
        "    Question/Prompt: {prompt}\n",
        "    \n",
        "    Information from search:\n",
        "    {' '.join(search_contents)}\n",
        "    \n",
        "    Please provide a response that incorporates information from these sources, and include citations in the format [Source X] where X is the source number.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Generate response\n",
        "    response = model.generate_content(grounded_prompt)\n",
        "    \n",
        "    return response.text, search_results\n",
        "\n",
        "# Interactive loop for multiple prompts\n",
        "while True:\n",
        "    user_prompt = input(\"Enter your prompt (or 'quit' to exit): \")\n",
        "    if user_prompt.lower() == 'quit':\n",
        "        break\n",
        "    \n",
        "    response, sources = generate_grounded_content(user_prompt)\n",
        "    print(\"\\nResponse:\")\n",
        "    print(response)\n",
        "    print(\"\\nSources:\")\n",
        "    for i, source in enumerate(sources, 1):\n",
        "        print(f\"[Source {i}] {source}\")\n",
        "\n",
        "print(\"Thank you for using Gemini 1.5 Flash with search grounding!\")\n",
        "\n",
        "```\n",
        "\n",
        "I've updated the code to include functionality for grounding in Google search results and providing citations. Here's an explanation of the changes:\n",
        "\n",
        "1. We added the `googlesearch-python` library to perform web searches.\n",
        "2. We introduced new functions:\n",
        "   - `get_search_results()`: Performs a Google search and returns a list of URLs.\n",
        "   - `get_webpage_content()`: Fetches and extracts text content from a given URL.\n",
        "   - `generate_grounded_content()`: This function replaces the previous `generate_content()`. It performs a search based on the user's prompt, fetches content from the search results, and then uses this information to ground Gemini's response.\n",
        "\n",
        "3. The main loop now uses `generate_grounded_content()` and prints both the response and the sources used.\n",
        "\n",
        "To use this in Google Colab:\n",
        "\n",
        "1. Create a new notebook in Google Colab\n",
        "2. Copy and paste this updated code into a code cell\n",
        "3. Replace 'YOUR_API_KEY' with your actual Gemini API key\n",
        "4. Run the cell\n",
        "\n",
        "Please note the following:\n",
        "\n",
        "- This code uses the `googlesearch-python` library, which scrapes Google search results. Be aware that excessive use might lead to your IP being temporarily blocked by Google.\n",
        "- The code fetches only the first 1000 characters from each webpage to avoid overloading the model with too much input.\n",
        "- The response from Gemini will now include citations in the format [Source X], referring to the list of sources printed after the response.\n",
        "\n",
        "Remember that this is a basic implementation and might need further refinement depending on your specific needs. For instance, you might want to implement error handling, improve the webpage content extraction, or fine-tune the prompt engineering.\n",
        "\n",
        "Would you like me to explain any part of this updated code in more detail?\n"
      ],
      "metadata": {
        "id": "IlYjOvXiOfaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the required libraries\n",
        "!pip install -q -U google-generativeai googlesearch-python"
      ],
      "metadata": {
        "id": "n-T78DUI_I27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "747c93bb-2fba-4b75-c64e-b085d7540227"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.2/164.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m718.3/718.3 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the required libraries\n",
        "# !pip install google-generativeai googlesearch-python\n",
        "\n",
        "import google.generativeai as genai\n",
        "import os\n",
        "from googlesearch import search\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Set up your API key\n",
        "# Replace 'YOUR_API_KEY' with your actual Gemini API key\n",
        "os.environ['GOOGLE_API_KEY'] = 'YOUR_API_KEY'\n",
        "genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
        "\n",
        "# Initialize the model\n",
        "# model = genai.GenerativeModel('gemini-1.5-pro')\n",
        "model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "\n",
        "def get_search_results(query, num_results=5):\n",
        "    results = []\n",
        "    for j in search(query, num_results=num_results):\n",
        "        results.append(j)\n",
        "    return results\n",
        "\n",
        "def get_webpage_content(url):\n",
        "    try:\n",
        "        response = requests.get(url, timeout=5)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        return soup.get_text()[:1000]  # Get first 1000 characters\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "def generate_grounded_content(prompt):\n",
        "    # Get search results\n",
        "    search_results = get_search_results(prompt)\n",
        "\n",
        "    # Fetch content from search results\n",
        "    search_contents = [f\"Source {i+1}: {get_webpage_content(url)}\" for i, url in enumerate(search_results)]\n",
        "\n",
        "    # Combine prompt with search contents\n",
        "    grounded_prompt = f\"\"\"\n",
        "    Based on the following information, please answer the question or respond to the prompt:\n",
        "    Question/Prompt: {prompt}\n",
        "\n",
        "    Information from search:\n",
        "    {' '.join(search_contents)}\n",
        "\n",
        "    Please provide a response that incorporates information from these sources, and include citations in the format [Source X] where X is the source number.\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate response\n",
        "    response = model.generate_content(grounded_prompt)\n",
        "\n",
        "    return response.text, search_results\n",
        "\n",
        "# Interactive loop for multiple prompts\n",
        "while True:\n",
        "    user_prompt = input(\"Enter your prompt (or 'quit' to exit): \")\n",
        "    if user_prompt.lower() == 'quit':\n",
        "        break\n",
        "\n",
        "    response, sources = generate_grounded_content(user_prompt)\n",
        "    # print(\"\\nResponse:\")\n",
        "    # print(response)\n",
        "    display(Markdown(response))\n",
        "    # print(\"\\nSources:\")\n",
        "    display(Markdown(\"Sources:\"))\n",
        "    for i, source in enumerate(sources, 1):\n",
        "        # print(f\"[Source {i}] {source}\")\n",
        "        display(Markdown(f\"[Source {i}] {source}\"))\n",
        "\n",
        "print(\"Thank you for using Gemini 1.5 Flash with search grounding!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "p3lKVx9SkkJq",
        "outputId": "f1d9a4b1-4ed7-447c-c56b-ca91680c61f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your prompt (or 'quit' to exit): Act as an expert food critic and list out the top 5 Indian restaurants in NJ.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:400 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 483.29ms\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "BadRequest",
          "evalue": "400 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: API key not valid. Please pass a valid API key.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadRequest\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-c86d92a772bc>\u001b[0m in \u001b[0;36m<cell line: 57>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msources\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_grounded_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_prompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0;31m# print(\"\\nResponse:\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;31m# print(response)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-c86d92a772bc>\u001b[0m in \u001b[0;36mgenerate_grounded_content\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;31m# Generate response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrounded_prompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearch_results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/generativeai/generative_models.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[1;32m    329\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgeneration_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerateContentResponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m                 response = self._client.generate_content(\n\u001b[0m\u001b[1;32m    332\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m                     \u001b[0;34m**\u001b[0m\u001b[0mrequest_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m         \u001b[0;31m# Send the request.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 827\u001b[0;31m         response = rpc(\n\u001b[0m\u001b[1;32m    828\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m             \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/gapic_v1/method.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maximum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiplier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multiplier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             )\n\u001b[0;32m--> 293\u001b[0;31m             return retry_target(\n\u001b[0m\u001b[1;32m    294\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predicate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;31m# defer to shared logic for handling errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             _retry_error_helper(\n\u001b[0m\u001b[1;32m    154\u001b[0m                 \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m                 \u001b[0mdeadline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/retry/retry_base.py\u001b[0m in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0moriginal_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         )\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfinal_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msource_exc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mon_error_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mon_error_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msleep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msleep_generator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misawaitable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ASYNC_RETRY_WARNING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/timeout.py\u001b[0m in \u001b[0;36mfunc_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"timeout\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime_since_first_attempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc_with_timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0merror_remapped_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_grpc_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/rest.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, request, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    844\u001b[0m             \u001b[0;31m# subclass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 846\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mcore_exceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_http_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m             \u001b[0;31m# Return the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBadRequest\u001b[0m: 400 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: API key not valid. Please pass a valid API key."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Prompt: Please add code for grounding by providing a pdf file.\n",
        "\n",
        "###Response:\n",
        "Certainly! I'll add functionality to ground Gemini's responses using content from a PDF file. We'll use the `PyPDF2` library to extract text from PDF files. This will allow users to upload a PDF and use its content as a knowledge base for Gemini's responses.\n",
        "\n",
        "Let's update our code to include this new functionality:\n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "# Install the required libraries\n",
        "!pip install google-generativeai googlesearch-python PyPDF2\n",
        "\n",
        "import google.generativeai as genai\n",
        "import os\n",
        "from googlesearch import search\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import PyPDF2\n",
        "from google.colab import files\n",
        "\n",
        "# Set up your API key\n",
        "# Replace 'YOUR_API_KEY' with your actual Gemini API key\n",
        "os.environ['GOOGLE_API_KEY'] = 'YOUR_API_KEY'\n",
        "genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
        "\n",
        "# Initialize the model\n",
        "model = genai.GenerativeModel('gemini-1.5-pro')\n",
        "\n",
        "def get_search_results(query, num_results=5):\n",
        "    results = []\n",
        "    for j in search(query, num_results=num_results):\n",
        "        results.append(j)\n",
        "    return results\n",
        "\n",
        "def get_webpage_content(url):\n",
        "    try:\n",
        "        response = requests.get(url, timeout=5)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        return soup.get_text()[:1000]  # Get first 1000 characters\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "def extract_text_from_pdf(file_path):\n",
        "    with open(file_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        text = \"\"\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "def generate_grounded_content(prompt, pdf_content=None):\n",
        "    if pdf_content:\n",
        "        grounded_prompt = f\"\"\"\n",
        "        Based on the following information from the provided PDF, please answer the question or respond to the prompt:\n",
        "        Question/Prompt: {prompt}\n",
        "        \n",
        "        Information from PDF:\n",
        "        {pdf_content[:3000]}  # Limit to first 3000 characters to avoid overloading\n",
        "        \n",
        "        Please provide a response that incorporates information from this PDF.\n",
        "        \"\"\"\n",
        "    else:\n",
        "        # Get search results\n",
        "        search_results = get_search_results(prompt)\n",
        "        \n",
        "        # Fetch content from search results\n",
        "        search_contents = [f\"Source {i+1}: {get_webpage_content(url)}\" for i, url in enumerate(search_results)]\n",
        "        \n",
        "        grounded_prompt = f\"\"\"\n",
        "        Based on the following information, please answer the question or respond to the prompt:\n",
        "        Question/Prompt: {prompt}\n",
        "        \n",
        "        Information from search:\n",
        "        {' '.join(search_contents)}\n",
        "        \n",
        "        Please provide a response that incorporates information from these sources, and include citations in the format [Source X] where X is the source number.\n",
        "        \"\"\"\n",
        "    \n",
        "    # Generate response\n",
        "    response = model.generate_content(grounded_prompt)\n",
        "    \n",
        "    return response.text, search_results if not pdf_content else None\n",
        "\n",
        "# Function to handle PDF upload\n",
        "def upload_pdf():\n",
        "    uploaded = files.upload()\n",
        "    if uploaded:\n",
        "        file_name = next(iter(uploaded))\n",
        "        return extract_text_from_pdf(file_name)\n",
        "    return None\n",
        "\n",
        "# Main interaction loop\n",
        "print(\"Welcome to Gemini 1.5 Flash with search and PDF grounding!\")\n",
        "pdf_content = None\n",
        "use_pdf = input(\"Do you want to upload a PDF for grounding? (yes/no): \").lower() == 'yes'\n",
        "\n",
        "if use_pdf:\n",
        "    print(\"Please upload your PDF file.\")\n",
        "    pdf_content = upload_pdf()\n",
        "    if pdf_content:\n",
        "        print(\"PDF uploaded and processed successfully!\")\n",
        "    else:\n",
        "        print(\"Failed to upload or process PDF. Falling back to search grounding.\")\n",
        "\n",
        "while True:\n",
        "    user_prompt = input(\"Enter your prompt (or 'quit' to exit): \")\n",
        "    if user_prompt.lower() == 'quit':\n",
        "        break\n",
        "    \n",
        "    response, sources = generate_grounded_content(user_prompt, pdf_content)\n",
        "    print(\"\\nResponse:\")\n",
        "    print(response)\n",
        "    \n",
        "    if sources:  # Only print sources if using search grounding\n",
        "        print(\"\\nSources:\")\n",
        "        for i, source in enumerate(sources, 1):\n",
        "            print(f\"[Source {i}] {source}\")\n",
        "\n",
        "print(\"Thank you for using Gemini 1.5 Flash with grounding!\")\n",
        "\n",
        "```\n",
        "\n",
        "I've updated the code to include functionality for grounding using a PDF file. Here's an explanation of the changes:\n",
        "\n",
        "1. We added the `PyPDF2` library to handle PDF files.\n",
        "2. We introduced new functions:\n",
        "   - `extract_text_from_pdf()`: Extracts text content from a given PDF file.\n",
        "   - `upload_pdf()`: Handles the PDF upload process in Google Colab.\n",
        "\n",
        "3. The `generate_grounded_content()` function has been modified to accept an optional `pdf_content` parameter. If PDF content is provided, it uses that for grounding instead of performing a web search.\n",
        "\n",
        "4. The main interaction loop now gives users the option to upload a PDF at the start. If a PDF is uploaded, it will be used for grounding all subsequent prompts.\n",
        "\n",
        "To use this in Google Colab:\n",
        "\n",
        "1. Create a new notebook in Google Colab\n",
        "2. Copy and paste this updated code into a code cell\n",
        "3. Replace 'YOUR_API_KEY' with your actual Gemini API key\n",
        "4. Run the cell\n",
        "\n",
        "When you run the code:\n",
        "1. You'll be asked if you want to upload a PDF for grounding.\n",
        "2. If you choose to upload a PDF, you'll be prompted to select a file from your local system.\n",
        "3. After the PDF is processed (or if you choose not to use a PDF), you can start entering prompts.\n",
        "4. The responses will be grounded in either the PDF content or web search results, depending on your choice.\n",
        "\n",
        "Please note:\n",
        "- The PDF grounding uses only the first 3000 characters of the PDF to avoid overloading the model. You may need to adjust this limit depending on your specific use case and the model's token limit.\n",
        "- If PDF grounding is used, the responses won't include web search citations.\n",
        "- This implementation assumes the PDF is primarily text-based. It may not work well with PDFs that are heavily image-based or have complex layouts.\n",
        "\n",
        "Would you like me to explain any part of this updated code in more detail?\n"
      ],
      "metadata": {
        "id": "xxoRdYa4OtNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the required libraries\n",
        "!pip install -q -U google-generativeai googlesearch-python PyPDF2"
      ],
      "metadata": {
        "id": "zfek70WRGGIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the required libraries\n",
        "# !pip install google-generativeai googlesearch-python PyPDF2\n",
        "\n",
        "import google.generativeai as genai\n",
        "import os\n",
        "from googlesearch import search\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import PyPDF2\n",
        "from google.colab import files\n",
        "\n",
        "# Set up your API key\n",
        "# Replace 'YOUR_API_KEY' with your actual Gemini API key\n",
        "# os.environ['GOOGLE_API_KEY'] = 'YOUR_API_KEY'\n",
        "genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
        "\n",
        "# Initialize the model\n",
        "# model = genai.GenerativeModel('gemini-1.5-pro')\n",
        "model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "\n",
        "def get_search_results(query, num_results=5):\n",
        "    results = []\n",
        "    for j in search(query, num_results=num_results):\n",
        "        results.append(j)\n",
        "    return results\n",
        "\n",
        "def get_webpage_content(url):\n",
        "    try:\n",
        "        response = requests.get(url, timeout=5)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        return soup.get_text()[:1000]  # Get first 1000 characters\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "def extract_text_from_pdf(file_path):\n",
        "    with open(file_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        text = \"\"\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "def generate_grounded_content(prompt, pdf_content=None):\n",
        "    if pdf_content:\n",
        "        grounded_prompt = f\"\"\"\n",
        "        Based on the following information from the provided PDF, please answer the question or respond to the prompt:\n",
        "        Question/Prompt: {prompt}\n",
        "\n",
        "        Information from PDF:\n",
        "        {pdf_content[:100000]}  # Limit to first 100000 characters to avoid overloading\n",
        "\n",
        "        Please provide a response that incorporates information from this PDF.\n",
        "        \"\"\"\n",
        "    else:\n",
        "        # Get search results\n",
        "        search_results = get_search_results(prompt)\n",
        "\n",
        "        # Fetch content from search results\n",
        "        search_contents = [f\"Source {i+1}: {get_webpage_content(url)}\" for i, url in enumerate(search_results)]\n",
        "\n",
        "        grounded_prompt = f\"\"\"\n",
        "        Based on the following information, please answer the question or respond to the prompt:\n",
        "        Question/Prompt: {prompt}\n",
        "\n",
        "        Information from search:\n",
        "        {' '.join(search_contents)}\n",
        "\n",
        "        Please provide a response that incorporates information from these sources, and include citations in the format [Source X] where X is the source number.\n",
        "        \"\"\"\n",
        "\n",
        "    # Generate response\n",
        "    response = model.generate_content(grounded_prompt)\n",
        "\n",
        "    return response.text, search_results if not pdf_content else None\n",
        "\n",
        "# Function to handle PDF upload\n",
        "def upload_pdf():\n",
        "    uploaded = files.upload()\n",
        "    if uploaded:\n",
        "        file_name = next(iter(uploaded))\n",
        "        return extract_text_from_pdf(file_name)\n",
        "    return None\n",
        "\n",
        "# Main interaction loop\n",
        "print(\"Welcome to Gemini 1.5 Flash with search and PDF grounding!\")\n",
        "pdf_content = None\n",
        "use_pdf = input(\"Do you want to upload a PDF for grounding? (yes/no): \").lower() == 'yes'\n",
        "\n",
        "if use_pdf:\n",
        "    print(\"Please upload your PDF file.\")\n",
        "    pdf_content = upload_pdf()\n",
        "    if pdf_content:\n",
        "        print(\"PDF uploaded and processed successfully!\")\n",
        "    else:\n",
        "        print(\"Failed to upload or process PDF. Falling back to search grounding.\")\n",
        "\n",
        "while True:\n",
        "    user_prompt = input(\"Enter your prompt (or 'quit' to exit): \")\n",
        "    if user_prompt.lower() == 'quit':\n",
        "        break\n",
        "\n",
        "    response, sources = generate_grounded_content(user_prompt, pdf_content)\n",
        "    print(\"\\nResponse:\")\n",
        "    print(response)\n",
        "\n",
        "    if sources:  # Only print sources if using search grounding\n",
        "        print(\"\\nSources:\")\n",
        "        for i, source in enumerate(sources, 1):\n",
        "            print(f\"[Source {i}] {source}\")\n",
        "\n",
        "print(\"Thank you for using Gemini 1.5 Flash with grounding!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 718
        },
        "id": "tjW06gNgB7ht",
        "outputId": "993709d5-c4d7-45fc-b343-9a780ed57a33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to Gemini 1.5 Flash with search and PDF grounding!\n",
            "Do you want to upload a PDF for grounding? (yes/no): yes\n",
            "Please upload your PDF file.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-00f00af9-cf3a-4b18-b0e1-8ea43c30a9cb\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-00f00af9-cf3a-4b18-b0e1-8ea43c30a9cb\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving EC2_migration.pdf to EC2_migration (1).pdf\n",
            "PDF uploaded and processed successfully!\n",
            "Enter your prompt (or 'quit' to exit): Please advice the diﬀerences between image import and instance import.\n",
            "\n",
            "Response:\n",
            "The key differences between image import and instance import are:\n",
            "\n",
            "**Image Import:**\n",
            "\n",
            "* **Import Result:** Creates an Amazon Machine Image (AMI)\n",
            "* **Process:**  Exports the VM in a supported format, uploads it to Amazon S3, then starts the import task.\n",
            "* **AMI Use:** The resulting AMI can be used to launch multiple EC2 instances.\n",
            "* **Flexibility:**  Allows for copying the AMI to other Regions and exporting the AMI back to a VM.\n",
            "* **CLI Support:** Uses the AWS CLI `import-image` command. \n",
            "* **Multi-disk support:** Supported.\n",
            "* **Windows BYOL support:** Supported.\n",
            "\n",
            "**Instance Import:**\n",
            "\n",
            "* **Import Result:** Imports the VM as a stopped EC2 instance.\n",
            "* **Process:** Exports the VM in a supported format, uploads it to Amazon S3, then starts the instance import task. \n",
            "* **Instance State:** The instance starts in a stopped state.\n",
            "* **AMI Creation:** You can create an AMI from the imported instance.\n",
            "* **Flexibility:** Allows for copying the resulting AMI to other Regions and exporting the instance back to your virtualization environment.\n",
            "* **CLI Support:**  Uses the deprecated Amazon EC2 CLI command `import-instance`  (not AWS CLI).\n",
            "* **Multi-disk support:** Supported.\n",
            "* **Windows BYOL support:** Not supported; must import as an AMI instead.\n",
            "\n",
            "**Recommendation:** \n",
            "\n",
            "The document strongly recommends importing VMs as AMIs instead of instances. This provides more flexibility and is supported by the modern AWS CLI. \n",
            "\n",
            "Enter your prompt (or 'quit' to exit): quit\n",
            "Thank you for using Gemini 1.5 Flash with grounding!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uDidj2iLGpGr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}