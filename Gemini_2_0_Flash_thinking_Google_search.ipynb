{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vrangayyan6/GenAI/blob/main/Gemini_2_0_Flash_thinking_Google_search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Using reasoning LLM (gemini-2.0-flash-thinking-exp) grounding with Google search results"
      ],
      "metadata": {
        "id": "4Uu69EJvazwC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "n-T78DUI_I27"
      },
      "outputs": [],
      "source": [
        "# Install the required libraries\n",
        "!pip install -q -U google-generativeai googlesearch-python"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the required libraries\n",
        "# !pip install google-generativeai googlesearch-python\n",
        "\n",
        "import google.generativeai as genai\n",
        "import os\n",
        "from googlesearch import search\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from google.colab import userdata\n",
        "\n",
        "# add your API key in Colab Secrets (left menu)\n",
        "genai.configure(api_key=userdata.get('GOOGLE_API_KEY'))"
      ],
      "metadata": {
        "id": "WEGjDdFrXMpq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for m in genai.list_models():\n",
        "  if 'generateContent' in m.supported_generation_methods:\n",
        "    print(m.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "id": "-pZtv4R3XL_Z",
        "outputId": "b9500950-dbd6-4594-e097-fc50838dcfff"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/gemini-1.0-pro-latest\n",
            "models/gemini-1.0-pro\n",
            "models/gemini-pro\n",
            "models/gemini-1.0-pro-001\n",
            "models/gemini-1.0-pro-vision-latest\n",
            "models/gemini-pro-vision\n",
            "models/gemini-1.5-pro-latest\n",
            "models/gemini-1.5-pro-001\n",
            "models/gemini-1.5-pro-002\n",
            "models/gemini-1.5-pro\n",
            "models/gemini-1.5-flash-latest\n",
            "models/gemini-1.5-flash-001\n",
            "models/gemini-1.5-flash-001-tuning\n",
            "models/gemini-1.5-flash\n",
            "models/gemini-1.5-flash-002\n",
            "models/gemini-1.5-flash-8b\n",
            "models/gemini-1.5-flash-8b-001\n",
            "models/gemini-1.5-flash-8b-latest\n",
            "models/gemini-1.5-flash-8b-exp-0827\n",
            "models/gemini-1.5-flash-8b-exp-0924\n",
            "models/gemini-2.0-flash-exp\n",
            "models/gemini-2.0-flash\n",
            "models/gemini-2.0-flash-001\n",
            "models/gemini-2.0-flash-lite-preview\n",
            "models/gemini-2.0-flash-lite-preview-02-05\n",
            "models/gemini-2.0-pro-exp\n",
            "models/gemini-2.0-pro-exp-02-05\n",
            "models/gemini-exp-1206\n",
            "models/gemini-2.0-flash-thinking-exp-01-21\n",
            "models/gemini-2.0-flash-thinking-exp\n",
            "models/gemini-2.0-flash-thinking-exp-1219\n",
            "models/learnlm-1.5-pro-experimental\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "p3lKVx9SkkJq"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Initialize the model\n",
        "model = genai.GenerativeModel('gemini-2.0-flash-thinking-exp')\n",
        "\n",
        "# get Google search results\n",
        "def get_search_results(query, num_results=5):\n",
        "    results = []\n",
        "    for j in search(query, num_results=num_results):\n",
        "        results.append(j)\n",
        "    return results\n",
        "\n",
        "# get webpage content of the Google search results\n",
        "def get_webpage_content(url):\n",
        "    try:\n",
        "        response = requests.get(url, timeout=5)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        return soup.get_text()[:10000]  # Get first 10000 characters\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "# generate response with webpage content of the Google search results\n",
        "def generate_grounded_content(prompt):\n",
        "    # Get search results\n",
        "    search_results = get_search_results(prompt)\n",
        "\n",
        "    # Fetch content from search results\n",
        "    search_contents = [f\"Source {i+1}: {get_webpage_content(url)}\" for i, url in enumerate(search_results)]\n",
        "\n",
        "    # Combine prompt with search contents\n",
        "    grounded_prompt = f\"\"\"\n",
        "    Based on the following information, please answer the question or respond to the prompt:\n",
        "    Question/Prompt: {prompt}\n",
        "\n",
        "    Information from search:\n",
        "    {' '.join(search_contents)}\n",
        "\n",
        "    Please provide a response that incorporates information from these sources, and include citations in the format [Source X] where X is the source number.\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate response\n",
        "    response = model.generate_content(grounded_prompt)\n",
        "\n",
        "    return response.text, search_results, grounded_prompt\n",
        "\n",
        "# Interactive loop for multiple prompts\n",
        "# while True:\n",
        "#     user_prompt = input(\"Enter your prompt (or 'quit' to exit): \")\n",
        "#     if user_prompt.lower() == 'quit':\n",
        "#         break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Provide the prompt"
      ],
      "metadata": {
        "id": "57bFWsz0aP6n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "uDidj2iLGpGr"
      },
      "outputs": [],
      "source": [
        "user_prompt = \"\"\"\n",
        "You are a Python expert specializing in implementing Retrieval-Augmented Generation (RAG) with cutting-edge AI models and tools. Write a Python script to achieve the following:\n",
        "\n",
        "1. Objective: Build a Retrieval-Augmented Generation (RAG) system using the Google Gemini 1.5 Flash model, Chroma as the vector database, and Streamlit for the user interface. The system should enable users to input a query, retrieve relevant context from a document database using Chroma, and generate a context-aware response using the Google Gemini 1.5 Flash model.\n",
        "\n",
        "2. Requirements:\n",
        "   - Document Ingestion with Chroma:\n",
        "     - Use Chroma to store and manage a set of documents.\n",
        "     - Read PDF files from a specified folder, extract text from the PDFs, and embed the content using a suitable text embedding model compatible with Chroma.\n",
        "   - Query Workflow:\n",
        "     - When a user inputs a query through the Streamlit interface, retrieve the top-k most relevant documents from Chroma.\n",
        "   - Integration with Google Gemini 1.5 Flash:\n",
        "     - Use the retrieved documents as context to generate a response from the Google Gemini 1.5 Flash model.\n",
        "   - Streamlit Interface:\n",
        "     - Create an intuitive web interface with:\n",
        "       - A file upload feature for PDFs, which will automatically update the Chroma database with the newly added content.\n",
        "       - A text input box for user queries.\n",
        "       - A display area for both the retrieved documents and the generated response.\n",
        "   - Modularity:\n",
        "     - Structure the code with clear modular functions, such as:\n",
        "       - Extracting text from PDFs.\n",
        "       - Embedding and storing documents in Chroma.\n",
        "       - Querying Chroma for relevant documents.\n",
        "       - Generating responses using Google Gemini 1.5 Flash.\n",
        "       - Streamlit app setup and interaction.\n",
        "\n",
        "3. Assumptions:\n",
        "   - Google Gemini 1.5 Flash API access is available and properly configured.\n",
        "   - Chroma library is installed and accessible.\n",
        "   - Streamlit and a PDF parsing library like PyPDF2 or pdfplumber are installed and set up.\n",
        "\n",
        "4. Additional Considerations:\n",
        "   - Include error handling for cases where no relevant documents are found.\n",
        "   - Provide comments to explain the purpose of each function and important lines of code.\n",
        "   - Ensure the code is compatible with Python 3.8+.\n",
        "   - Ensure uploaded PDFs are processed dynamically without requiring a server restart.\n",
        "\n",
        "Please generate the Python code for the complete implementation.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generate response"
      ],
      "metadata": {
        "id": "QvU0EiUgabnt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "response, sources, grounded_prompt = generate_grounded_content(user_prompt)\n",
        "\n",
        "print(\"\\nPrompt:\")\n",
        "print(grounded_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kfa7JsH6YWFy",
        "outputId": "8722d25e-a80f-4c0a-df98-4c396bc0d208"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Prompt:\n",
            "\n",
            "    Based on the following information, please answer the question or respond to the prompt:\n",
            "    Question/Prompt: \n",
            "You are a Python expert specializing in implementing Retrieval-Augmented Generation (RAG) with cutting-edge AI models and tools. Write a Python script to achieve the following:\n",
            "\n",
            "1. Objective: Build a Retrieval-Augmented Generation (RAG) system using the Google Gemini 1.5 Flash model, Chroma as the vector database, and Streamlit for the user interface. The system should enable users to input a query, retrieve relevant context from a document database using Chroma, and generate a context-aware response using the Google Gemini 1.5 Flash model.\n",
            "\n",
            "2. Requirements:\n",
            "   - Document Ingestion with Chroma:\n",
            "     - Use Chroma to store and manage a set of documents.\n",
            "     - Read PDF files from a specified folder, extract text from the PDFs, and embed the content using a suitable text embedding model compatible with Chroma.\n",
            "   - Query Workflow:\n",
            "     - When a user inputs a query through the Streamlit interface, retrieve the top-k most relevant documents from Chroma.\n",
            "   - Integration with Google Gemini 1.5 Flash:\n",
            "     - Use the retrieved documents as context to generate a response from the Google Gemini 1.5 Flash model.\n",
            "   - Streamlit Interface:\n",
            "     - Create an intuitive web interface with:\n",
            "       - A file upload feature for PDFs, which will automatically update the Chroma database with the newly added content.\n",
            "       - A text input box for user queries.\n",
            "       - A display area for both the retrieved documents and the generated response.\n",
            "   - Modularity:\n",
            "     - Structure the code with clear modular functions, such as:\n",
            "       - Extracting text from PDFs.\n",
            "       - Embedding and storing documents in Chroma.\n",
            "       - Querying Chroma for relevant documents.\n",
            "       - Generating responses using Google Gemini 1.5 Flash.\n",
            "       - Streamlit app setup and interaction.\n",
            "\n",
            "3. Assumptions:\n",
            "   - Google Gemini 1.5 Flash API access is available and properly configured.\n",
            "   - Chroma library is installed and accessible.\n",
            "   - Streamlit and a PDF parsing library like PyPDF2 or pdfplumber are installed and set up.\n",
            "\n",
            "4. Additional Considerations:\n",
            "   - Include error handling for cases where no relevant documents are found.\n",
            "   - Provide comments to explain the purpose of each function and important lines of code.\n",
            "   - Ensure the code is compatible with Python 3.8+.\n",
            "   - Ensure uploaded PDFs are processed dynamically without requiring a server restart.\n",
            "\n",
            "Please generate the Python code for the complete implementation.\n",
            "\n",
            "\n",
            "    Information from search:\n",
            "    Source 1: Python RAG Tutorial (with Local LLMs): AI For Your PDFs - YouTubeAboutPressCopyrightContact usCreatorsAdvertiseDevelopersTermsPrivacyPolicy & SafetyHow YouTube worksTest new featuresNFL Sunday Ticket© 2025 Google LLC Source 2: \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Build a Retrieval Augmented Generation (RAG) App: Part 1 | 🦜️🔗 LangChain\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Skip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1💬SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain's stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem🦜🛠️ LangSmith🦜🕸️ LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyTutorialsBuild a Retrieval Augmented Generation (RAG) App: Part 1On this pageBuild a Retrieval Augmented Generation (RAG) App: Part 1\n",
            "One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.\n",
            "This is a multi-part tutorial:\n",
            "\n",
            "Part 1 (this guide) introduces RAG and walks through a minimal implementation.\n",
            "Part 2 extends the implementation to accommodate conversation-style interactions and multi-step retrieval processes.\n",
            "\n",
            "This tutorial will show how to build a simple Q&A application\n",
            "over a text data source. Along the way we’ll go over a typical Q&A\n",
            "architecture and highlight additional resources for more advanced Q&A techniques. We’ll also see\n",
            "how LangSmith can help us trace and understand our application.\n",
            "LangSmith will become increasingly helpful as our application grows in\n",
            "complexity.\n",
            "If you're already familiar with basic retrieval, you might also be interested in\n",
            "this high-level overview of different retrieval techniques.\n",
            "Note: Here we focus on Q&A for unstructured data. If you are interested for RAG over structured data, check out our tutorial on doing question/answering over SQL data.\n",
            "Overview​\n",
            "A typical RAG application has two main components:\n",
            "Indexing: a pipeline for ingesting data from a source and indexing it. This usually happens offline.\n",
            "Retrieval and generation: the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\n",
            "Note: the indexing portion of this tutorial will largely follow the semantic search tutorial.\n",
            "The most common full sequence from raw data to answer looks like:\n",
            "Indexing​\n",
            "\n",
            "Load: First we need to load our data. This is done with Document Loaders.\n",
            "Split: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won't fit in a model's finite context window.\n",
            "Store: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a VectorStore and Embeddings model.\n",
            "\n",
            "\n",
            "Retrieval and generation​\n",
            "\n",
            "Retrieve: Given a user input, relevant splits are retrieved from storage using a Retriever.\n",
            "Generate: A ChatModel / LLM produces an answer using a prompt that includes both the question with the retrieved data\n",
            "\n",
            "\n",
            "Once we've indexed our data, we will use LangGraph as  Source 3: \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "What is Retrieval-Augmented Generation (RAG)?\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "DeepSeek\n",
            "\n",
            "\n",
            "Learning Paths\n",
            "\n",
            "\n",
            "GenAI Pinnacle Program\n",
            "\n",
            "\n",
            "Agentic AI Pioneer Program\n",
            "New\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Login\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                Switch Mode\n",
            "                            \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                Logout\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Interview PrepCareerGenAIPrompt EnggChatGPTLLMLangchainRAGAI AgentsMachine LearningDeep LearningGenAI ToolsLLMOpsPythonNLPSQLAIML Projects \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Home\n",
            "\n",
            "Artificial Intelligence                            \n",
            "\n",
            "                                What is Retrieval-Augmented Generation (RAG)?                                \n",
            "\n",
            "\n",
            "\n",
            "What is Retrieval-Augmented Generation (RAG)?\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                            Soumyadarshan Dash                             \n",
            " Last Updated : \n",
            "                                06 Feb, 2025 \n",
            "                            \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  13  min read\n",
            "                        \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Retrieval-Augmented Generation (RAG) is a smart AI technique that combines two powerful tools: information retrieval and text generation. Imagine a system that can search for relevant facts or data (like a librarian) and then use that information to create clear, accurate, and detailed answers (like a writer). RAG is used in chatbots, virtual assistants, and other AI tools to provide better, more informed responses. It’s like having a super-smart helper that knows how to find and use the right information. In this article, you will get to know all about Retrieval-Augmented Generation, its uses, applications, and how it will shape the future of RAG and LLMs.\n",
            "This article was published as a part of the Data Science Blogathon.\n",
            "Table of contentsWhat is Retrieval-Augmented Generation (RAG)?Why Use RAG?The Fusion of Retrieval-Based and Generative ModelsRange of Data Sources to Empower RAG ModelsBenefits of Retrieval-Augmented Generation (RAG)Enhanced LLM MemoryImproved ContextualizationUpdatable MemorySource CitationsReduced HallucinationsDiverse Approaches in RAGEthical Considerations in RAGApplications of Retrieval Augmented Generation (RAG)The Future of RAGs and LLMsHow Does Retrieval-Augmented Generation Work?Utilizing LangChain for Enhanced Retrieval-Augmented Generation (RAG)Installation of LangChain and OpenAI LibrariesWeb Data Loading for the RAG Knowledge BaseSplit the Data into ChunksEmbedding and Vector Store SetupEstablishing the Retrieval SystemInitializes the RAG SystemIssue Queries to the RAG SystemRetrieves ResponsesFrequently Asked Questions\n",
            "What is Retrieval-Augmented Generation (RAG)?\n",
            "Retrieval-Augmented Generation, or RAG, represents a cutting-edge approach to artificial intelligence (AI) and natural language processing (NLP). At its core, RAG LLM is an innovative framework that combines the strengths of retrieval-based and generative models, revolutionizing how AI systems understand and generate human-like text.\n",
            "\n",
            "\n",
            "\n",
            "Why Use RAG? \n",
            "The development of RAG is a direct response to the limitations of large language models (LLMs) like GPT. While LLMs have shown impressive text generation capabilities, they often struggle to provide contextually relevant responses, hindering their utility in practical applications. RAG LLM aims to bridge this gap by offering a solution that understands user intent and delivers meaningful and context-aware replies.\n",
            "The Fusion of Retrieval-Based and Generative Models\n",
            "RAG is fundamentally a hybrid model that seamlessly integrates two critical components. Retrieval-based methods involve accessing and extracting information from external knowledge sources such as databases, articles, or websites. \n",
            "On the other hand, generative models excel in generating coherent and contextually relevant text. What distinguishes RAG Model is its ability to harmonize these two components, creating a symbiotic relationship that allows it to comprehend user queries deeply and produce responses that are not just accurate but also contextually rich.\n",
            "Deconstructing RAG’s Mechanics\n",
            "To grasp the essence of RAG LLM, it’s essential to deconstruct its operational mechanics. RAG operates through a series of well-defined steps:\n",
            "\n",
            "Begin by receiving and processing user input.\n",
            "Analyze the user input to understand its meaning and intent.\n",
            "Utilize retrieval-based methods to access external knowledge sources. This enriches the understanding of the user’s query.\n",
            "Use the retrieved external knowledge to enhance comprehension.\n",
            "Employ generative capabilities to craft responses. Ensure responses are factually accurate, contextually relevant, and coherent.\n",
            "Combine all the information gathered to produce responses that are meaningful and human-like.\n",
            "Ensure that the transformation of user queries into responses is done effectively.\n",
            "\n",
            "Checkout this article about the 12 Generative AI Models to Explore in 2025\n",
            "The Role of Language Models and User Input\n",
            "Central to understanding RAG is appreciating the role of Large Language Models (LLMs) in AI systems. LLMs like GPT are the backbone of many NLP applications, including chatbots and virtual assistants. They excel in processing user input and generating text, but their accuracy and contextual awareness are paramount for successful interactions. RAG strives to enhance these essential aspects by integrating retrieval and generation.\n",
            "Incorporating External Knowledge Sources\n",
            "RAG’s distinguishing feature is its ability to integrate external knowledge sources seamlessly. By drawing from vast information repositories, RAG augments its understanding, enabling it to provide well-informed and contextually nuanced responses. Incorporating external knowledge elevates the quality of interactions and ensures that users receive relevant and accurate information.\n",
            "Generating Contextual Responses\n",
            "Ultimately, RAG’s hallmark is its ability to generate contextual responses. Moreover, it considers the broader context of user queries, leverages external knowledge, and produces responses demonstrating a deep understanding of the user’s needs. Consequently, these context-aware responses are a significant advancement, as they facilitate more natural and human-like interactions, making AI systems powered by RAG highly effective in various domains.\n",
            "Retrieval Augmented Generation (RAG) is a transformative concept in AI and NLP. Additionally, by harmonizing retrieval and generation components, RAG addresses the limitations of existing language models and paves the way for more intelligent and context-aware AI interactions. Furthermore, its ability to seamlessly integrate external knowledge sources and generate responses that align with user intent positions RAG as a game-changer in developing AI systems that can truly understand and communicate with users in a human-like manner.\n",
            "Range of Data Sources to Empower RAG Models\n",
            "In this section, we delve into the pivotal role of external data sources within the Retrieval Augmented Generation (RAG) framework. We explore the diverse range of data sources that can be harnessed to empower RAG-driven models.\n",
            "APIs and Real-time Databases\n",
            "APIs (Application Programming Interfaces) and real-time databases are dynamic sources that provide up-to-the-minute information to RAG-driven models. They also allow models to access the latest data as it becomes available.\n",
            "Document Repositories\n",
            "Document repositories serve as valuable knowledge stores, offering structured and unstructured information. Additionally, they are fundamental in expanding the knowledge base that RAG models can draw upon.\n",
            "Webpages and Scraping\n",
            "Web scraping is a method for extracting information from web pages. It enables RAG LLM models to access dynamic web content, making it a crucial source for real-time data retrieval.\n",
            "Databases and Structured Information\n",
            "Databases provide structured data that can be queried and extracted. Additionally, RAG models can utilize databases to retrieve specific information, enhancing their responses’ accuracy.\n",
            "Benefits of Retrieval-Augmented Generation (RAG)\n",
            "Let us now talk about the benefits of Retrieval Augmented Generation or RAG Model.\n",
            "Enhanced LLM Memory\n",
            "RAG addresses the information capacity limitation of traditional Language Models (LLMs). Traditional LLMs have a limited memory called “Parametric memory.” RAG introduces a “Non-Parametric memory” by tapping into external knowledge sources. This significantly expands the knowledge base of LLMs, enabling them to provide more comprehensive and accurate responses.\n",
            "Improved Contextualization\n",
            "RAG enhances the contextual understanding of LLMs by retrieving and integrating relevant contextual documents. This empowers the model to generate responses that align seamlessly with the specific context of the user’s input, resulting in accurate and contextually appropriate outputs.\n",
            "Updatable Memory\n",
            "A standout advantage of RAG is its ability to accommodate real-time updates and fresh sources without extensive model retraining. Moreover, this keeps the external knowledge base current and ensures that LLM-generated responses are always based on the latest and most relevant information.\n",
            "Source Citations\n",
            "RAG-equipped models can provide sources for their responses, thereby enhancing transparency and credibility. Moreover, users can access the sources that inform the LLM’s responses, promoting transparency and trust in AI-generated content.\n",
            "Reduced Hallucinations\n",
            "Studies have shown that RAG models exhibit fewer hallucinations and higher response accuracy. They are also less likely to leak sensitive information. Reduced hallucinations and increased accuracy make RAG models more reliable in generating content.\n",
            "These benefits collectively make Retrieval Augmented Generation (RAG) a transformative framework in Natural Language Processing. Consequently, it overcomes the limitations of traditional language models and enhances the capabilities of AI-powered applications.\n",
            "Diverse Approaches in RAG\n",
            "RAG Model offers a spectrum of approac Source 4: Implementing RAG (Retrieval-Augmented Generation) in Python | by Raahil Modi 🌿 | Python in Plain EnglishOpen in appSign upSign inWriteSign upSign inMember-only storyImplementing RAG (Retrieval-Augmented Generation) in PythonA Technical GuideRaahil Modi 🌿·FollowPublished inPython in Plain English·3 min read·Aug 13, 2024--ShareRetrieval-Augmented Generation (RAG)Retrieval-Augmented Generation (RAG) is a powerful technique that enhances language models by combining them with a retrieval system. This allows the model to access and utilize external knowledge when generating responses.In this tutorial, we’ll walk through implementing a basic RAG system in Python.PrerequisitesBefore we begin, make sure you have the following libraries installed:pip install transformers faiss-cpu sentence-transformers torchStep 1: Setting Up the Knowledge BaseFirst, we’ll create a simple knowledge base using a list of documents. In a real-world scenario, this could be a much larger dataset.documents = [    \"The capital of France is Paris.\",    \"Python is a popular programming language.\",    \"The Eiffel Tower is located in Paris.\",    \"Machine learning is a subset of artificial intelligence.\",    \"RAG stands for Retrieval-Augmented Generation.\"]Step 2: Implementing the RetrieverWe’ll use FAISS (Facebook AI Similarity Search) for efficient similarity search and SentenceTransformers for encoding our…----FollowPublished in Python in Plain English40K Followers·Last published 3 days agoNew Python content every day. Follow to join our 3.5M+ monthly readers.FollowFollowWritten by Raahil Modi 🌿234 Followers·60 FollowingExplorer of life's endless possibilities.FollowNo responses yetHelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " Source 5: \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "How to Become a RAG Specialist in 2025? - Analytics Vidhya\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "DeepSeek\n",
            "\n",
            "\n",
            "Learning Paths\n",
            "\n",
            "\n",
            "GenAI Pinnacle Program\n",
            "\n",
            "\n",
            "Agentic AI Pioneer Program\n",
            "New\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Login\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                Switch Mode\n",
            "                            \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                Logout\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Interview PrepCareerGenAIPrompt EnggChatGPTLLMLangchainRAGAI AgentsMachine LearningDeep LearningGenAI ToolsLLMOpsPythonNLPSQLAIML Projects \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Home\n",
            "\n",
            "RAG                            \n",
            "\n",
            "                                How to Become a RAG Specialist in 2025?                                \n",
            "\n",
            "\n",
            "\n",
            "How to Become a RAG Specialist in 2025?\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                            Pankaj Singh                             \n",
            " Last Updated : \n",
            "                                16 Dec, 2024 \n",
            "                            \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  26  min read\n",
            "                        \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "What does it take to become a specialist in a particular skill? It is said, the learner should invest around 10,000 hours of focused practice to gain expertise in a field. But in this fast-paced world, where time is the most valuable thing, we need to work smarter to plan how a beginner can get a strong hold on a tech-specific skill in a limited time. The answer lies in having a clear Learning Path or a perfect Roadmap. It Worked for Me! Today, I am going to talk about how you can become a RAG Specialist, and I will provide a detailed roadmap for diving into the world of Retrieval Augmented Generation (RAG).\n",
            "RAG Specialist Roadmap is for: \n",
            "\n",
            "Python developers & ML Engineers who want to build AI-driven applications leveraging LLMs and custom enterprise data. \n",
            "Students and Learners willing to dive into RAG implementations and gain hands-on experience with practical examples.\n",
            "\n",
            "Table of contentsWhat is RAG, and Where is it Used?How RAG Works?Learning Path to Become a RAG SpecialistStep 1. Programming Language ProficiencyStep 2. Core Libraries and ToolsStep 3. Foundations of Machine Learning and Deep Learning – with a Focus on Information RetrievalStep 4. Natural Language Processing (NLP)Step 5. Introduction to RAG SystemsStep 6. Retrieval-Augmented Generation (RAG) ArchitectureStep 7. Information Retrieval (IR)Step 8. Building Retrieval SystemsStep 9. Integration into RAG SystemsStep 10. RAG EvaluationStep 11. RAG Challenges and ImprovementsStep 12. Practical ImplementationStep 13. Advanced RAGStep 14. Ongoing Learning and ResourcesStep 15. Community and Continuous LearningStep 16. Hands-On Capstone ProjectRAG Research PapersConclusionFrequently Asked Questions\n",
            "\n",
            "\n",
            "Click here to download the RAG Specialist roadmap!\n",
            "What is RAG, and Where is it Used?\n",
            "\n",
            "RAG (Retrieval-Augmented Generation) is a technique that enhances the performance of language models by combining them with an external retrieval mechanism. This allows the model to pull in relevant information from large document stores or knowledge bases at inference time, improving the quality and factual accuracy of its generated responses.\n",
            "Key Components of RAG:\n",
            "\n",
            "Retrieval Component: A retriever (typically based on similarity search) scans a large corpus of documents or databases to find relevant passages based on a query.\n",
            "Generation Component:\n",
            "\n",
            "After retrieving the relevant documents or passages, a language model (e.g., GPT-4o, Claude 3.5, Gemini 1.5, Llama 3.2) uses these passages as context to generate a more informed response or output.\n",
            "The model can either generate a direct answer or summarize the retrieved information depending on the task.\n",
            "\n",
            "\n",
            "\n",
            "The main advantage of RAG is that it allows the model to handle long-tail knowledge and tasks that require factual accuracy or specialized knowledge, which might not be directly encoded in the model’s parameters.\n",
            "Also read: Top 8 Applications of RAGs in Workplaces.\n",
            "How RAG Works?\n",
            "Here’s how RAG works:\n",
            "\n",
            "When a query or prompt is received, the system first retrieves relevant documents or information from a pre-indexed corpus (such as Wikipedia, product catalogs, research papers, etc.).\n",
            "The language model then uses the retrieved information to generate a response.\n",
            "The model might perform multiple retrieval steps (iterative retrieval) or use a combination of different retrieval techniques to improve the quality of the retrieved documents.\n",
            "\n",
            "To know more about this, refer to this article: What is Retrieval-Augmented Generation (RAG)?                  Build a RAG Pipeline With the LLama Index.\n",
            "\n",
            "\n",
            "\n",
            "Learning Path to Become a RAG Specialist\n",
            "To become an RAG specialist, you’ll need to gain expertise in multiple areas, ranging from foundational knowledge in machine learning and natural language processing (NLP) to hands-on experience with RAG-specific architectures and tools. Below is a comprehensive learning path tailored to guide you through this journey to becoming an RAG Specialist:\n",
            "Step 1. Programming Language Proficiency\n",
            "Master the primary programming languages used in Retrieval-Augmented Generation (RAG) development, with a strong focus on Python.\n",
            "Languages:\n",
            "\n",
            "Python: The dominant language in AI/ML research and development. Python is widely used for data science, machine learning, natural language processing (NLP), and creating systems that rely on RAG methods. Its simplicity, combined with an extensive ecosystem of libraries, makes it the go-to choice for AI and ML tasks.\n",
            "\n",
            "Key Skills:\n",
            "\n",
            "Data structures (lists, dictionaries, sets, tuples).\n",
            "File handling (text, JSON, CSV).\n",
            "Exception handling and debugging.\n",
            "Object-oriented programming (OOP) and functional programming concepts.\n",
            "Writing modular and reusable code.\n",
            "\n",
            "Resources:\n",
            "\n",
            "“Automate the Boring Stuff with Python” by Al Sweigart – A great resource for beginners that covers Python basics with real-world applications, focusing on practical scripting for automation and productivity.\n",
            "“Python Crash Course” by Eric Matthes – A beginner-friendly book that offers a comprehensive introduction to Python, covering all essential topics and providing hands-on projects to build your skills.\n",
            "\n",
            "For more books: \n",
            "\n",
            "15 Best Python Books For You\n",
            "8 Popular Tools for RAG Applications\n",
            "\n",
            "Step 2. Core Libraries and Tools\n",
            "Gain familiarity with the libraries and tools crucial for building and deploying Retrieval-Augmented Generation (RAG) systems. These libraries help streamline the process of data processing, data retrieval, model development, natural language processing (NLP), and integration with large-scale systems.\n",
            "Key Libraries\n",
            "\n",
            "Machine Learning & Deep Learning:\n",
            "\n",
            "TensorFlow, PyTorch (for training and deploying models).\n",
            "Scikit-learn (for preprocessing and auxiliary tasks).\n",
            "\n",
            "\n",
            "NLP-Specific:\n",
            "\n",
            "Hugging Face Transformers (pretrained models like GPT-4o, Claude 3.5, Gemini 1.5, Llama 3.2).\n",
            "SpaCy and NLTK (text preprocessing and linguistic features).\n",
            "\n",
            "\n",
            "Data Processing:\n",
            "\n",
            "Pandas (data manipulation).\n",
            "NumPy (numerical computing).\n",
            "PyTorch Lightning (scalable ML workflows).\n",
            "PyTorch Litserve\n",
            "\n",
            "\n",
            "\n",
            "Resources\n",
            "\n",
            "Official documentation for TensorFlow, PyTorch, Hugging Face, SpaCy, and other libraries.\n",
            "GitHub repositories for RAG-specific frameworks (e.g., Haystack, PyTorch Lightning, listserve, LangChain and LlamaIndex).\n",
            "Online tutorials and courses (e.g., Analytics Vidhya, Deeplearning.ai, Coursera, edX, Fast.ai) covering deep learning, NLP, and RAG development.\n",
            "Course on Python: Introduction to Python\n",
            "\n",
            "Also explore: Coding Essentials Course\n",
            "Step 3. Foundations of Machine Learning and Deep Learning – with a Focus on Information Retrieval\n",
            "The foundations of Machine Learning and Deep Learning in RAG (Retriever-Augmented Generation) is to equip learners with the essential knowledge of machine learning and deep learning techniques. This involves understanding model architectures, data retrieval methods, and the integration of generative models with information retrieval systems to enhance the accuracy and efficiency of AI-driven responses and tasks.\n",
            "Key Topics:\n",
            "\n",
            "Supervised Learning: Learning from labeled data to predict outcomes (e.g., regression and classification).\n",
            "Unsupervised Learning: Identifying patterns and structures in unlabeled data (e.g., clustering and dimensionality reduction).\n",
            "Reinforcement Learning: Learning by interacting with an environment and receiving feedback through rewards or penalties.\n",
            "Core Algorithms:\n",
            "\n",
            "Linear Regression: For predicting continuous outcomes.\n",
            "Logistic Regression: For binary classification.\n",
            "Decision Trees: For decision-making based on data features.\n",
            "K-Means Clustering: For grouping similar data points.\n",
            "K-Nearest Neighbors (KNN): For classification based on proximity to labeled data points.\n",
            "\n",
            "\n",
            "Information Retrieval (IR) Systems: Information Retrieval refers to the process of obtaining relevant information from large datasets or databases, typically in response to a query. The core components include:\n",
            "\n",
            "Search Engine Basics:\n",
            "\n",
            "Indexing: Involves creating an index of all documents in a corpus to facilitate fast retrieval based on the search terms.\n",
            "Query Processing: When a user enters a query, the system processes it, matches it to relevant documents in the index, and ranks the documents based on relevance.\n",
            "Ranking Algorithms: Ranking is typically based on algorithms like TF-IDF (Term Frequency-Inverse Document Frequency), which measures the importance of a term in a document relative to its occurrence in the entire corpus.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Vector Space Model (VSM): Documents and queries are represented as vectors in a multi-dimensional space, where each dimension represents a term. The similarity between a query and a document is determined using measures like Cosine Similarity.\n",
            "Latent Semantic Analysis (LSA): A technique used to reduce dimensionality and capture deeper semantic relationships between terms and documents through Singular Value Decomposition (SVD).\n",
            "BM25, Cosine Similarity and PageRank for ranking document relevance.\n",
            "Clustering: Clustering is a type of unsupervised learning where data points are grouped in\n",
            "\n",
            "    Please provide a response that incorporates information from these sources, and include citations in the format [Source X] where X is the source number.\n",
            "    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nResponse:\")\n",
        "print(response)\n",
        "# display(Markdown(response))\n",
        "print(\"\\nSources:\")\n",
        "# display(Markdown(\"Sources:\"))\n",
        "for i, source in enumerate(sources, 1):\n",
        "    print(f\"[Source {i}] {source}\")\n",
        "    # display(Markdown(f\"[Source {i}] {source}\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ete-zUH5ZY3s",
        "outputId": "06bba801-633e-4d40-ca79-ffad7703f4d4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Response:\n",
            "```python\n",
            "import streamlit as st\n",
            "import os\n",
            "import pdfplumber\n",
            "import chromadb\n",
            "from chromadb.utils import embedding_functions\n",
            "import google.generativeai as genai\n",
            "\n",
            "# Placeholder for your Gemini API key - REPLACE THIS WITH YOUR ACTUAL API KEY or use Streamlit secrets\n",
            "GOOGLE_API_KEY = \"YOUR_API_KEY\"\n",
            "genai.configure(api_key=GOOGLE_API_KEY)\n",
            "\n",
            "# Select the Gemini 1.5 Flash model\n",
            "MODEL_NAME = 'gemini-1.5-flash'\n",
            "gemini_model = genai.GenerativeModel(MODEL_NAME)\n",
            "\n",
            "# ChromaDB setup\n",
            "CHROMA_PERSIST_DIR = \"chroma_db\"\n",
            "client = chromadb.PersistentClient(path=CHROMA_PERSIST_DIR)\n",
            "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L6-v2\") # Efficient and good for Chroma\n",
            "collection_name = \"rag_collection\"\n",
            "\n",
            "try:\n",
            "    collection = client.get_collection(name=collection_name, embedding_function=sentence_transformer_ef)\n",
            "    print(f\"Collection '{collection_name}' loaded successfully.\")\n",
            "except ValueError:\n",
            "    collection = client.create_collection(name=collection_name, embedding_function=sentence_transformer_ef)\n",
            "    print(f\"Collection '{collection_name}' created.\")\n",
            "\n",
            "\n",
            "# 1. Function to extract text from a PDF file\n",
            "def extract_text_from_pdf(pdf_path):\n",
            "    \"\"\"\n",
            "    Extracts text content from a PDF file.\n",
            "\n",
            "    Args:\n",
            "        pdf_path (str): The path to the PDF file.\n",
            "\n",
            "    Returns:\n",
            "        str: Extracted text content, or None if extraction fails.\n",
            "    \"\"\"\n",
            "    text = \"\"\n",
            "    try:\n",
            "        with pdfplumber.open(pdf_path) as pdf:\n",
            "            for page in pdf.pages:\n",
            "                text += page.extract_text() + \"\\n\" # Add newline for page separation\n",
            "        return text\n",
            "    except Exception as e:\n",
            "        st.error(f\"Error extracting text from {pdf_path}: {e}\")\n",
            "        return None\n",
            "\n",
            "# 2. Function to embed and store documents in Chroma\n",
            "def embed_and_store_documents(pdf_files):\n",
            "    \"\"\"\n",
            "    Embeds text from PDF files and stores them in ChromaDB.\n",
            "\n",
            "    Args:\n",
            "        pdf_files (list): List of uploaded PDF files from Streamlit.\n",
            "    \"\"\"\n",
            "    if not pdf_files:\n",
            "        st.warning(\"No PDF files uploaded for processing.\")\n",
            "        return\n",
            "\n",
            "    st.info(\"Processing and embedding PDFs...\")\n",
            "    for pdf_file in pdf_files:\n",
            "        try:\n",
            "            temp_pdf_path = f\"temp_{pdf_file.name}\" # Create a temporary file path\n",
            "            with open(temp_pdf_path, \"wb\") as f:\n",
            "                f.write(pdf_file.read()) # Save uploaded file to temp path\n",
            "\n",
            "            text_content = extract_text_from_pdf(temp_pdf_path)\n",
            "            if text_content:\n",
            "                # Split text into smaller chunks for better retrieval (optional, but recommended)\n",
            "                text_chunks = text_content.split(\"\\n\\n\") # Simple split by double newline, adjust as needed\n",
            "\n",
            "                ids = [pdf_file.name + f\"_chunk_{i}\" for i in range(len(text_chunks))] # Unique IDs for chunks\n",
            "                metadatas = [{\"source\": pdf_file.name, \"chunk\": i} for i in range(len(text_chunks))] # Metadata for each chunk\n",
            "\n",
            "                collection.add(\n",
            "                    embeddings=None, # Embeddings are handled by the embedding function\n",
            "                    documents=text_chunks,\n",
            "                    metadatas=metadatas,\n",
            "                    ids=ids\n",
            "                )\n",
            "                st.success(f\"'{pdf_file.name}' processed and added to the database.\")\n",
            "            os.remove(temp_pdf_path) # Clean up temporary file\n",
            "        except Exception as e:\n",
            "            st.error(f\"Error processing '{pdf_file.name}': {e}\")\n",
            "\n",
            "    st.info(\"PDF processing and embedding complete.\")\n",
            "\n",
            "\n",
            "# 3. Function to query Chroma for relevant documents\n",
            "def query_chroma(query, k=3):\n",
            "    \"\"\"\n",
            "    Queries ChromaDB for the top-k most relevant documents based on a user query.\n",
            "\n",
            "    Args:\n",
            "        query (str): The user query.\n",
            "        k (int): The number of top documents to retrieve.\n",
            "\n",
            "    Returns:\n",
            "        list: A list of retrieved document texts (strings).\n",
            "    \"\"\"\n",
            "    if not query:\n",
            "        return []\n",
            "\n",
            "    results = collection.query(\n",
            "        query_texts=[query],\n",
            "        n_results=k\n",
            "    )\n",
            "    retrieved_documents = results['documents'][0] if results['documents'] else []\n",
            "    return retrieved_documents\n",
            "\n",
            "\n",
            "# 4. Function to generate response using Google Gemini 1.5 Flash\n",
            "def generate_response(query, context_documents):\n",
            "    \"\"\"\n",
            "    Generates a context-aware response using Google Gemini 1.5 Flash.\n",
            "\n",
            "    Args:\n",
            "        query (str): The user query.\n",
            "        context_documents (list): List of retrieved document texts to use as context.\n",
            "\n",
            "    Returns:\n",
            "        str: The generated response from Gemini 1.5 Flash.\n",
            "    \"\"\"\n",
            "    if not context_documents:\n",
            "        return \"No relevant documents found to answer your query.\"\n",
            "\n",
            "    context = \"\\n\\n\".join(context_documents) # Combine context documents into a single string\n",
            "    prompt_content = f\"\"\"Answer the question based on the context provided.\n",
            "    Context:\n",
            "    {context}\n",
            "\n",
            "    Question: {query}\n",
            "    \"\"\"\n",
            "\n",
            "    try:\n",
            "        response = gemini_model.generate_content(prompt_content)\n",
            "        return response.text\n",
            "    except Exception as e:\n",
            "        st.error(f\"Error generating response from Gemini: {e}\")\n",
            "        return \"Error generating response. Please try again.\"\n",
            "\n",
            "\n",
            "# 5. Streamlit App Setup and Interaction\n",
            "def streamlit_app():\n",
            "    st.title(\"RAG System with Gemini 1.5 Flash, Chroma, and Streamlit\")\n",
            "\n",
            "    st.header(\"Document Upload\")\n",
            "    uploaded_pdfs = st.file_uploader(\"Upload PDF documents\", type=[\"pdf\"], accept_multiple_files=True)\n",
            "\n",
            "    if uploaded_pdfs:\n",
            "        if st.button(\"Process PDFs and Update Database\"):\n",
            "            embed_and_store_documents(uploaded_pdfs)\n",
            "\n",
            "    st.header(\"Query & Response\")\n",
            "    user_query = st.text_input(\"Enter your query:\")\n",
            "\n",
            "    if st.button(\"Get Response\"):\n",
            "        if user_query:\n",
            "            with st.spinner(\"Retrieving relevant documents and generating response...\"):\n",
            "                retrieved_docs = query_chroma(user_query)\n",
            "\n",
            "                st.subheader(\"Retrieved Documents (Context):\")\n",
            "                if retrieved_docs:\n",
            "                    for i, doc in enumerate(retrieved_docs):\n",
            "                        st.markdown(f\"**Document {i+1}:**\")\n",
            "                        st.write(doc) # Display document text directly\n",
            "                        st.markdown(\"---\")\n",
            "                else:\n",
            "                    st.info(\"No relevant documents found for this query.\")\n",
            "\n",
            "                gemini_response = generate_response(user_query, retrieved_docs)\n",
            "\n",
            "                st.subheader(\"Generated Response:\")\n",
            "                st.write(gemini_response)\n",
            "        else:\n",
            "            st.warning(\"Please enter a query.\")\n",
            "\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    streamlit_app()\n",
            "```\n",
            "\n",
            "**Explanation:**\n",
            "\n",
            "1.  **Import Libraries:** Imports necessary libraries: `streamlit` for UI, `os` for file operations, `pdfplumber` for PDF text extraction, `chromadb` for vector database, `google.generativeai` for Gemini API.\n",
            "2.  **API Key and Model Setup:**\n",
            "    *   **`GOOGLE_API_KEY = \"YOUR_API_KEY\"`**:  **Important:** Replace `\"YOUR_API_KEY\"` with your actual Google Gemini API key. You can also use Streamlit secrets management for better security.\n",
            "    *   `genai.configure(api_key=GOOGLE_API_KEY)`: Configures the Gemini API client with your API key.\n",
            "    *   `MODEL_NAME = 'gemini-1.5-flash'` and `gemini_model = genai.GenerativeModel(MODEL_NAME)`: Selects and initializes the Gemini 1.5 Flash model.\n",
            "3.  **ChromaDB Setup:**\n",
            "    *   `CHROMA_PERSIST_DIR = \"chroma_db\"`:  Sets the directory where ChromaDB will store its data.\n",
            "    *   `client = chromadb.PersistentClient(...)`: Initializes a persistent Chroma client, meaning the database will be saved to disk.\n",
            "    *   `sentence_transformer_ef = ...`: Creates an embedding function using Sentence Transformers (`all-MiniLM-L6-v2`). This model is efficient and works well with Chroma.\n",
            "    *   `collection_name = \"rag_collection\"`: Sets the name of the Chroma collection.\n",
            "    *   The code attempts to load an existing collection or creates a new one if it doesn't exist.\n",
            "4.  **`extract_text_from_pdf(pdf_path)` Function:**\n",
            "    *   Takes the path to a PDF file as input.\n",
            "    *   Uses `pdfplumber` to open the PDF and extract text page by page.\n",
            "    *   Returns the extracted text or `None` if there's an error.\n",
            "5.  **`embed_and_store_documents(pdf_files)` Function:**\n",
            "    *   Takes a list of uploaded PDF files from Streamlit.\n",
            "    *   Iterates through each PDF file:\n",
            "        *   Creates a temporary file path to save the uploaded file.\n",
            "        *   Saves the uploaded PDF content to the temporary file.\n",
            "        *   Calls `extract_text_from_pdf` to get the text content.\n",
            "        *   Splits the text into chunks (simple split by double newline - you might want to use more sophisticated splitting for real-world scenarios).\n",
            "        *   Generates unique IDs and metadata for each chunk.\n",
            "        *   Adds the text chunks, metadata, and IDs to the Chroma collection. Embeddings are automatically generated by the `sentence_transformer_ef` when adding documents to Chroma.\n",
            "        *   Deletes the temporary PDF file.\n",
            "6.  **`query_chroma(query, k=3)` Function:**\n",
            "    *   Takes a user query and the number of top documents to retrieve (`k`) as input.\n",
            "    *   Uses `collection.query` to search Chroma for documents relevant to the query.\n",
            "    *   Returns a list of retrieved document texts.\n",
            "7.  **`generate_response(query, context_documents)` Function:**\n",
            "    *   Takes the user query and a list of retrieved context documents.\n",
            "    *   Combines the context documents into a single string.\n",
            "    *   Constructs a prompt for Gemini 1.5 Flash, including the context and the user's question.\n",
            "    *   Uses `gemini_model.generate_content(prompt_content)` to get a response from Gemini.\n",
            "    *   Returns the generated response text.\n",
            "8.  **`streamlit_app()` Function:**\n",
            "    *   Sets up the Streamlit UI:\n",
            "        *   **Title:** \"RAG System with Gemini 1.5 Flash, Chroma, and Streamlit\"\n",
            "        *   **Document Upload Section:**  `st.file_uploader` for uploading PDF files. A \"Process PDFs and Update Database\" button triggers the `embed_and_store_documents` function.\n",
            "        *   **Query & Response Section:** `st.text_input` for the user to enter a query. A \"Get Response\" button triggers the query and response generation process.\n",
            "        *   **Output Display:**\n",
            "            *   Displays \"Retrieved Documents (Context)\" and lists the retrieved document texts.\n",
            "            *   Displays \"Generated Response\" and shows the response from Gemini 1.5 Flash.\n",
            "    *   Uses `st.spinner` to show a loading animation while processing the query and generating the response.\n",
            "    *   Includes basic error handling and warnings in the UI.\n",
            "\n",
            "**To Run the Script:**\n",
            "\n",
            "1.  **Install Libraries:**\n",
            "    ```bash\n",
            "    pip install streamlit pdfplumber chromadb google-generativeai sentence-transformers\n",
            "    ```\n",
            "2.  **Get a Google Gemini API Key:** If you don't have one already, you'll need to get a Google Gemini API key from Google AI Studio ([https://makersuite.google.com/app/apikey](https://makersuite.google.com/app/apikey)).\n",
            "3.  **Replace Placeholder API Key:**  Replace `\"YOUR_API_KEY\"` in the script with your actual API key.\n",
            "4.  **Run the Streamlit App:**\n",
            "    ```bash\n",
            "    streamlit run your_script_name.py\n",
            "    ```\n",
            "    (Replace `your_script_name.py` with the actual name of your Python file).\n",
            "\n",
            "**Key improvements and considerations in this code:**\n",
            "\n",
            "*   **Modularity:**  Code is well-structured into functions, making it easier to understand, maintain, and extend.\n",
            "*   **Error Handling:** Includes basic error handling for PDF extraction and Gemini API calls.\n",
            "*   **Comments:**  Code is well-commented to explain the purpose of different sections.\n",
            "*   **Dynamic PDF Processing:** Uploaded PDFs are processed immediately when you click \"Process PDFs and Update Database,\" updating the Chroma database dynamically without requiring a server restart.\n",
            "*   **Clear UI:** Streamlit UI is designed to be intuitive with separate sections for document upload, query input, and output display.\n",
            "*   **Context Display:**  Retrieved documents are displayed in the UI so the user can see the context used for response generation.\n",
            "*   **Efficient Embeddings:** Uses `all-MiniLM-L6-v2` Sentence Transformer model, which is a good balance of efficiency and performance.\n",
            "*   **Chunking:** Implements basic text chunking, which is important for handling longer documents and improving retrieval relevance (though you might want to explore more advanced chunking strategies).\n",
            "*   **Temporary File Handling:** Uses temporary files to process uploaded PDFs and cleans them up afterwards.\n",
            "\n",
            "**Further Enhancements:**\n",
            "\n",
            "*   **More Advanced Text Splitting:**  Implement more sophisticated text splitting methods (e.g., recursive character splitting, semantic splitting) using libraries like Langchain's `RecursiveCharacterTextSplitter` or similar.\n",
            "*   **Metadata Management:**  Enhance metadata to include page numbers, document titles, etc., for better context and source tracking.\n",
            "*   **Reranking:**  Implement a reranking step after retrieval to further improve the relevance of retrieved documents (e.g., using Cohere Rerank, Sentence Transformers Cross-encoders).\n",
            "*   **Prompt Engineering:**  Experiment with different prompt templates for Gemini 1.5 Flash to optimize response quality.\n",
            "*   **User Feedback and Evaluation:** Add features for user feedback on response quality and implement evaluation metrics to assess RAG system performance.\n",
            "*   **Handling No Relevant Documents:** Improve the \"No relevant documents found\" case by potentially suggesting related topics or offering to search a wider knowledge base.\n",
            "*   **Asynchronous Processing:** For larger document sets or more complex RAG pipelines, consider using asynchronous processing to improve performance and responsiveness.\n",
            "*   **Security:** For production applications, implement proper API key management (using Streamlit secrets or environment variables) and consider other security aspects.\n",
            "\n",
            "Sources:\n",
            "[Source 1] https://www.youtube.com/watch?v=2TJxpyO3ei4\n",
            "[Source 2] https://python.langchain.com/docs/tutorials/rag/\n",
            "[Source 3] https://www.analyticsvidhya.com/blog/2023/09/retrieval-augmented-generation-rag-in-ai/\n",
            "[Source 4] https://python.plainenglish.io/implementing-rag-retrieval-augmented-generation-in-python-b239cee7dd55\n",
            "[Source 5] https://www.analyticsvidhya.com/blog/2024/12/rag-specialist/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kTYpvehweuxh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOgfRyWNCt2Vciv6D6rSNp1",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}