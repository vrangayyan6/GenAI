{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vrangayyan6/GenAI/blob/main/Gemini_2_0_Flash_thinking_Google_search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Using reasoning LLM (gemini-2.0-flash-thinking-exp) grounding with Google search results\n",
        "\n",
        "Create API key at https://ai.google.dev/"
      ],
      "metadata": {
        "id": "4Uu69EJvazwC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-T78DUI_I27"
      },
      "outputs": [],
      "source": [
        "# Install the required libraries\n",
        "!pip install -q -U google-generativeai googlesearch-python"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the required libraries\n",
        "# !pip install google-generativeai googlesearch-python\n",
        "\n",
        "import google.generativeai as genai\n",
        "import os\n",
        "from googlesearch import search\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from google.colab import userdata\n",
        "\n",
        "# Create API key https://ai.google.dev/\n",
        "# add your API key in Colab Secrets (left menu)\n",
        "genai.configure(api_key=userdata.get('GOOGLE_API_KEY'))"
      ],
      "metadata": {
        "id": "WEGjDdFrXMpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for m in genai.list_models():\n",
        "  if 'generateContent' in m.supported_generation_methods:\n",
        "    print(m.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        },
        "id": "-pZtv4R3XL_Z",
        "outputId": "60679173-595e-4835-f69c-42faa20c5516"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/gemini-1.0-pro-latest\n",
            "models/gemini-1.0-pro\n",
            "models/gemini-pro\n",
            "models/gemini-1.0-pro-001\n",
            "models/gemini-1.0-pro-vision-latest\n",
            "models/gemini-pro-vision\n",
            "models/gemini-1.5-pro-latest\n",
            "models/gemini-1.5-pro-001\n",
            "models/gemini-1.5-pro-002\n",
            "models/gemini-1.5-pro\n",
            "models/gemini-1.5-flash-latest\n",
            "models/gemini-1.5-flash-001\n",
            "models/gemini-1.5-flash-001-tuning\n",
            "models/gemini-1.5-flash\n",
            "models/gemini-1.5-flash-002\n",
            "models/gemini-1.5-flash-8b\n",
            "models/gemini-1.5-flash-8b-001\n",
            "models/gemini-1.5-flash-8b-latest\n",
            "models/gemini-1.5-flash-8b-exp-0827\n",
            "models/gemini-1.5-flash-8b-exp-0924\n",
            "models/gemini-2.0-flash-exp\n",
            "models/gemini-2.0-flash\n",
            "models/gemini-2.0-flash-001\n",
            "models/gemini-2.0-flash-lite-preview\n",
            "models/gemini-2.0-flash-lite-preview-02-05\n",
            "models/gemini-2.0-pro-exp\n",
            "models/gemini-2.0-pro-exp-02-05\n",
            "models/gemini-exp-1206\n",
            "models/gemini-2.0-flash-thinking-exp-01-21\n",
            "models/gemini-2.0-flash-thinking-exp\n",
            "models/gemini-2.0-flash-thinking-exp-1219\n",
            "models/learnlm-1.5-pro-experimental\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3lKVx9SkkJq"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Initialize the model\n",
        "model = genai.GenerativeModel('gemini-2.0-flash-thinking-exp')\n",
        "\n",
        "# get Google search results\n",
        "def get_search_results(query, num_results=5):\n",
        "    results = []\n",
        "    for j in search(query, num_results=num_results):\n",
        "        results.append(j)\n",
        "    return results\n",
        "\n",
        "# get webpage content of the Google search results\n",
        "def get_webpage_content(url):\n",
        "    try:\n",
        "        response = requests.get(url, timeout=5)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        return soup.get_text()[:10000]  # Get first 10000 characters\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "# generate response with webpage content of the Google search results\n",
        "def generate_grounded_content(prompt):\n",
        "    # Get search results\n",
        "    search_results = get_search_results(prompt)\n",
        "\n",
        "    # Fetch content from search results\n",
        "    search_contents = [f\"Source {i+1}: {get_webpage_content(url)}\" for i, url in enumerate(search_results)]\n",
        "\n",
        "    # Combine prompt with search contents\n",
        "    grounded_prompt = f\"\"\"\n",
        "    Based on the following information, please answer the question or respond to the prompt:\n",
        "    Question/Prompt: {prompt}\n",
        "\n",
        "    Information from search:\n",
        "    {' '.join(search_contents)}\n",
        "\n",
        "    Please provide a response that incorporates information from these sources, and include citations in the format [Source X] where X is the source number.\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate response\n",
        "    response = model.generate_content(grounded_prompt)\n",
        "\n",
        "    return response.text, search_results, grounded_prompt\n",
        "\n",
        "# Interactive loop for multiple prompts\n",
        "# while True:\n",
        "#     user_prompt = input(\"Enter your prompt (or 'quit' to exit): \")\n",
        "#     if user_prompt.lower() == 'quit':\n",
        "#         break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Provide the prompt"
      ],
      "metadata": {
        "id": "57bFWsz0aP6n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDidj2iLGpGr"
      },
      "outputs": [],
      "source": [
        "user_prompt = \"\"\"\n",
        "You are a Python expert specializing in implementing Retrieval-Augmented Generation (RAG) with cutting-edge AI models and tools. Write a Python script to achieve the following:\n",
        "\n",
        "1. Objective: Build a Retrieval-Augmented Generation (RAG) system using the Google Gemini 1.5 Flash model, Chroma as the vector database, and Streamlit for the user interface. The system should enable users to input a query, retrieve relevant context from a document database using Chroma, and generate a context-aware response using the Google Gemini 1.5 Flash model.\n",
        "\n",
        "2. Requirements:\n",
        "   - Document Ingestion with Chroma:\n",
        "     - Use Chroma to store and manage a set of documents.\n",
        "     - Read PDF files from a specified folder, extract text from the PDFs, and embed the content using a suitable text embedding model compatible with Chroma.\n",
        "   - Query Workflow:\n",
        "     - When a user inputs a query through the Streamlit interface, retrieve the top-k most relevant documents from Chroma.\n",
        "   - Integration with Google Gemini 1.5 Flash:\n",
        "     - Use the retrieved documents as context to generate a response from the Google Gemini 1.5 Flash model.\n",
        "   - Streamlit Interface:\n",
        "     - Create an intuitive web interface with:\n",
        "       - A file upload feature for PDFs, which will automatically update the Chroma database with the newly added content.\n",
        "       - A text input box for user queries.\n",
        "       - A display area for both the retrieved documents and the generated response.\n",
        "   - Modularity:\n",
        "     - Structure the code with clear modular functions, such as:\n",
        "       - Extracting text from PDFs.\n",
        "       - Embedding and storing documents in Chroma.\n",
        "       - Querying Chroma for relevant documents.\n",
        "       - Generating responses using Google Gemini 1.5 Flash.\n",
        "       - Streamlit app setup and interaction.\n",
        "\n",
        "3. Assumptions:\n",
        "   - Google Gemini 1.5 Flash API access is available and properly configured.\n",
        "   - Chroma library is installed and accessible.\n",
        "   - Streamlit and a PDF parsing library like PyPDF2 or pdfplumber are installed and set up.\n",
        "\n",
        "4. Additional Considerations:\n",
        "   - Include error handling for cases where no relevant documents are found.\n",
        "   - Provide comments to explain the purpose of each function and important lines of code.\n",
        "   - Ensure the code is compatible with Python 3.8+.\n",
        "   - Ensure uploaded PDFs are processed dynamically without requiring a server restart.\n",
        "\n",
        "Please generate the Python code for the complete implementation.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Grounded Prompt\n",
        "\n",
        "See the webpage contents from Google search results that are provided as input for the prompt."
      ],
      "metadata": {
        "id": "QvU0EiUgabnt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "response, sources, grounded_prompt = generate_grounded_content(user_prompt)\n",
        "\n",
        "print(\"\\nPrompt:\")\n",
        "print(grounded_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kfa7JsH6YWFy",
        "outputId": "d6188bb1-d0f9-43ad-e5dd-7fbadf9a7aea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Prompt:\n",
            "\n",
            "    Based on the following information, please answer the question or respond to the prompt:\n",
            "    Question/Prompt: \n",
            "You are a Python expert specializing in implementing Retrieval-Augmented Generation (RAG) with cutting-edge AI models and tools. Write a Python script to achieve the following:\n",
            "\n",
            "1. Objective: Build a Retrieval-Augmented Generation (RAG) system using the Google Gemini 1.5 Flash model, Chroma as the vector database, and Streamlit for the user interface. The system should enable users to input a query, retrieve relevant context from a document database using Chroma, and generate a context-aware response using the Google Gemini 1.5 Flash model.\n",
            "\n",
            "2. Requirements:\n",
            "   - Document Ingestion with Chroma:\n",
            "     - Use Chroma to store and manage a set of documents.\n",
            "     - Read PDF files from a specified folder, extract text from the PDFs, and embed the content using a suitable text embedding model compatible with Chroma.\n",
            "   - Query Workflow:\n",
            "     - When a user inputs a query through the Streamlit interface, retrieve the top-k most relevant documents from Chroma.\n",
            "   - Integration with Google Gemini 1.5 Flash:\n",
            "     - Use the retrieved documents as context to generate a response from the Google Gemini 1.5 Flash model.\n",
            "   - Streamlit Interface:\n",
            "     - Create an intuitive web interface with:\n",
            "       - A file upload feature for PDFs, which will automatically update the Chroma database with the newly added content.\n",
            "       - A text input box for user queries.\n",
            "       - A display area for both the retrieved documents and the generated response.\n",
            "   - Modularity:\n",
            "     - Structure the code with clear modular functions, such as:\n",
            "       - Extracting text from PDFs.\n",
            "       - Embedding and storing documents in Chroma.\n",
            "       - Querying Chroma for relevant documents.\n",
            "       - Generating responses using Google Gemini 1.5 Flash.\n",
            "       - Streamlit app setup and interaction.\n",
            "\n",
            "3. Assumptions:\n",
            "   - Google Gemini 1.5 Flash API access is available and properly configured.\n",
            "   - Chroma library is installed and accessible.\n",
            "   - Streamlit and a PDF parsing library like PyPDF2 or pdfplumber are installed and set up.\n",
            "\n",
            "4. Additional Considerations:\n",
            "   - Include error handling for cases where no relevant documents are found.\n",
            "   - Provide comments to explain the purpose of each function and important lines of code.\n",
            "   - Ensure the code is compatible with Python 3.8+.\n",
            "   - Ensure uploaded PDFs are processed dynamically without requiring a server restart.\n",
            "\n",
            "Please generate the Python code for the complete implementation.\n",
            "\n",
            "\n",
            "    Information from search:\n",
            "    Source 1: \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Build a Retrieval Augmented Generation (RAG) App: Part 1 | 🦜️🔗 LangChain\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Skip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1💬SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain's stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem🦜🛠️ LangSmith🦜🕸️ LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyTutorialsBuild a Retrieval Augmented Generation (RAG) App: Part 1On this pageBuild a Retrieval Augmented Generation (RAG) App: Part 1\n",
            "One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.\n",
            "This is a multi-part tutorial:\n",
            "\n",
            "Part 1 (this guide) introduces RAG and walks through a minimal implementation.\n",
            "Part 2 extends the implementation to accommodate conversation-style interactions and multi-step retrieval processes.\n",
            "\n",
            "This tutorial will show how to build a simple Q&A application\n",
            "over a text data source. Along the way we’ll go over a typical Q&A\n",
            "architecture and highlight additional resources for more advanced Q&A techniques. We’ll also see\n",
            "how LangSmith can help us trace and understand our application.\n",
            "LangSmith will become increasingly helpful as our application grows in\n",
            "complexity.\n",
            "If you're already familiar with basic retrieval, you might also be interested in\n",
            "this high-level overview of different retrieval techniques.\n",
            "Note: Here we focus on Q&A for unstructured data. If you are interested for RAG over structured data, check out our tutorial on doing question/answering over SQL data.\n",
            "Overview​\n",
            "A typical RAG application has two main components:\n",
            "Indexing: a pipeline for ingesting data from a source and indexing it. This usually happens offline.\n",
            "Retrieval and generation: the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\n",
            "Note: the indexing portion of this tutorial will largely follow the semantic search tutorial.\n",
            "The most common full sequence from raw data to answer looks like:\n",
            "Indexing​\n",
            "\n",
            "Load: First we need to load our data. This is done with Document Loaders.\n",
            "Split: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won't fit in a model's finite context window.\n",
            "Store: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a VectorStore and Embeddings model.\n",
            "\n",
            "\n",
            "Retrieval and generation​\n",
            "\n",
            "Retrieve: Given a user input, relevant splits are retrieved from storage using a Retriever.\n",
            "Generate: A ChatModel / LLM produces an answer using a prompt that includes both the question with the retrieved data\n",
            "\n",
            "\n",
            "Once we've indexed our data, we will use LangGraph as  Source 2: How to Create a Retrieval-Augmented Generation (RAG) System and Pipeline Using DeepSeek, LangChain and Ollama | by Joel Wembo | Feb, 2025 | Level Up CodingOpen in appSign upSign inWriteSign upSign inMember-only storyHow to Create a Retrieval-Augmented Generation (RAG) System and Pipeline Using DeepSeek, LangChain and OllamaUnlocking the Power of RAG: Integrating Local RAG using DeepSeek-r1, Ollama, FAISS, and LangChain for Smarter AIJoel Wembo·FollowPublished inLevel Up Coding·7 min read·2 days ago--ShareHow to Create a Retrieval-Augmented Generation (RAG) System and Pipeline Using DeepSeek R1, LangChain and OllamaIf you are not a medium member use the following friendly linkA Technical Guide to DeepSeek: Unlocking Advanced AI-Powered InsightsDeepSeek: A Chinese startup has built a low-cost AI model using less technologically advanced chips.python.plainenglish.ioRetrieval-Augmented Generation (RAG) systems combine the power of retrieval-based models and generative AI to deliver accurate and context-aware responses. This article provides a comprehensive guide to building a RAG system using Ollama, Python, and LangChain. We’ll walk through the setup of a local RAG pipeline, demonstrate how to integrate FAISS for efficient retrieval, and showcase how to generate responses using the DeepSeek R1 model.By combining retrieval mechanisms with generative models, RAG systems can provide more accurate, context-aware, and factually grounded responses. This article is designed for developers and AI practitioners who want to harness the power of RAG systems using Ollama, a…----FollowPublished in Level Up Coding203K Followers·Last published 1 day agoCoding tutorials and news. The developer homepage gitconnected.com && skilled.dev && levelup.devFollowFollowWritten by Joel Wembo1.2K Followers·285 FollowingCloud Solutions Architect @ prodxcloud. Expert in Django, AWS, Azure, EKS, Serverless Computing & Terraform. https://www.linkedin.com/in/joelotepawemboFollowNo responses yetHelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " Source 3: RAG 101: Introduction to Retrieval-Augmented Generation with Python - YouTubeAboutPressCopyrightContact usCreatorsAdvertiseDevelopersTermsPrivacyPolicy & SafetyHow YouTube worksTest new featuresNFL Sunday Ticket© 2025 Google LLC Source 4: You've been blocked by network security.To continue, log in to your Reddit account or use your developer tokenIf you think you've been blocked by mistake, file a ticket below and we'll look into it.Log in File a ticket Source 5: \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "What is Retrieval-Augmented Generation (RAG)?\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "DeepSeek\n",
            "\n",
            "\n",
            "Learning Paths\n",
            "\n",
            "\n",
            "GenAI Pinnacle Program\n",
            "\n",
            "\n",
            "Agentic AI Pioneer Program\n",
            "New\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Login\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                Switch Mode\n",
            "                            \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                Logout\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Interview PrepCareerGenAIPrompt EnggChatGPTLLMLangchainRAGAI AgentsMachine LearningDeep LearningGenAI ToolsLLMOpsPythonNLPSQLAIML Projects \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Home\n",
            "\n",
            "Artificial Intelligence                            \n",
            "\n",
            "                                What is Retrieval-Augmented Generation (RAG)?                                \n",
            "\n",
            "\n",
            "\n",
            "What is Retrieval-Augmented Generation (RAG)?\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                            Soumyadarshan Dash                             \n",
            " Last Updated : \n",
            "                                06 Feb, 2025 \n",
            "                            \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  13  min read\n",
            "                        \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Retrieval-Augmented Generation (RAG) is a smart AI technique that combines two powerful tools: information retrieval and text generation. Imagine a system that can search for relevant facts or data (like a librarian) and then use that information to create clear, accurate, and detailed answers (like a writer). RAG is used in chatbots, virtual assistants, and other AI tools to provide better, more informed responses. It’s like having a super-smart helper that knows how to find and use the right information. In this article, you will get to know all about Retrieval-Augmented Generation, its uses, applications, and how it will shape the future of RAG and LLMs.\n",
            "This article was published as a part of the Data Science Blogathon.\n",
            "Table of contentsWhat is Retrieval-Augmented Generation (RAG)?Why Use RAG?The Fusion of Retrieval-Based and Generative ModelsRange of Data Sources to Empower RAG ModelsBenefits of Retrieval-Augmented Generation (RAG)Enhanced LLM MemoryImproved ContextualizationUpdatable MemorySource CitationsReduced HallucinationsDiverse Approaches in RAGEthical Considerations in RAGApplications of Retrieval Augmented Generation (RAG)The Future of RAGs and LLMsHow Does Retrieval-Augmented Generation Work?Utilizing LangChain for Enhanced Retrieval-Augmented Generation (RAG)Installation of LangChain and OpenAI LibrariesWeb Data Loading for the RAG Knowledge BaseSplit the Data into ChunksEmbedding and Vector Store SetupEstablishing the Retrieval SystemInitializes the RAG SystemIssue Queries to the RAG SystemRetrieves ResponsesFrequently Asked Questions\n",
            "What is Retrieval-Augmented Generation (RAG)?\n",
            "Retrieval-Augmented Generation, or RAG, represents a cutting-edge approach to artificial intelligence (AI) and natural language processing (NLP). At its core, RAG LLM is an innovative framework that combines the strengths of retrieval-based and generative models, revolutionizing how AI systems understand and generate human-like text.\n",
            "\n",
            "\n",
            "\n",
            "Why Use RAG? \n",
            "The development of RAG is a direct response to the limitations of large language models (LLMs) like GPT. While LLMs have shown impressive text generation capabilities, they often struggle to provide contextually relevant responses, hindering their utility in practical applications. RAG LLM aims to bridge this gap by offering a solution that understands user intent and delivers meaningful and context-aware replies.\n",
            "The Fusion of Retrieval-Based and Generative Models\n",
            "RAG is fundamentally a hybrid model that seamlessly integrates two critical components. Retrieval-based methods involve accessing and extracting information from external knowledge sources such as databases, articles, or websites. \n",
            "On the other hand, generative models excel in generating coherent and contextually relevant text. What distinguishes RAG Model is its ability to harmonize these two components, creating a symbiotic relationship that allows it to comprehend user queries deeply and produce responses that are not just accurate but also contextually rich.\n",
            "Deconstructing RAG’s Mechanics\n",
            "To grasp the essence of RAG LLM, it’s essential to deconstruct its operational mechanics. RAG operates through a series of well-defined steps:\n",
            "\n",
            "Begin by receiving and processing user input.\n",
            "Analyze the user input to understand its meaning and intent.\n",
            "Utilize retrieval-based methods to access external knowledge sources. This enriches the understanding of the user’s query.\n",
            "Use the retrieved external knowledge to enhance comprehension.\n",
            "Employ generative capabilities to craft responses. Ensure responses are factually accurate, contextually relevant, and coherent.\n",
            "Combine all the information gathered to produce responses that are meaningful and human-like.\n",
            "Ensure that the transformation of user queries into responses is done effectively.\n",
            "\n",
            "Checkout this article about the 12 Generative AI Models to Explore in 2025\n",
            "The Role of Language Models and User Input\n",
            "Central to understanding RAG is appreciating the role of Large Language Models (LLMs) in AI systems. LLMs like GPT are the backbone of many NLP applications, including chatbots and virtual assistants. They excel in processing user input and generating text, but their accuracy and contextual awareness are paramount for successful interactions. RAG strives to enhance these essential aspects by integrating retrieval and generation.\n",
            "Incorporating External Knowledge Sources\n",
            "RAG’s distinguishing feature is its ability to integrate external knowledge sources seamlessly. By drawing from vast information repositories, RAG augments its understanding, enabling it to provide well-informed and contextually nuanced responses. Incorporating external knowledge elevates the quality of interactions and ensures that users receive relevant and accurate information.\n",
            "Generating Contextual Responses\n",
            "Ultimately, RAG’s hallmark is its ability to generate contextual responses. Moreover, it considers the broader context of user queries, leverages external knowledge, and produces responses demonstrating a deep understanding of the user’s needs. Consequently, these context-aware responses are a significant advancement, as they facilitate more natural and human-like interactions, making AI systems powered by RAG highly effective in various domains.\n",
            "Retrieval Augmented Generation (RAG) is a transformative concept in AI and NLP. Additionally, by harmonizing retrieval and generation components, RAG addresses the limitations of existing language models and paves the way for more intelligent and context-aware AI interactions. Furthermore, its ability to seamlessly integrate external knowledge sources and generate responses that align with user intent positions RAG as a game-changer in developing AI systems that can truly understand and communicate with users in a human-like manner.\n",
            "Range of Data Sources to Empower RAG Models\n",
            "In this section, we delve into the pivotal role of external data sources within the Retrieval Augmented Generation (RAG) framework. We explore the diverse range of data sources that can be harnessed to empower RAG-driven models.\n",
            "APIs and Real-time Databases\n",
            "APIs (Application Programming Interfaces) and real-time databases are dynamic sources that provide up-to-the-minute information to RAG-driven models. They also allow models to access the latest data as it becomes available.\n",
            "Document Repositories\n",
            "Document repositories serve as valuable knowledge stores, offering structured and unstructured information. Additionally, they are fundamental in expanding the knowledge base that RAG models can draw upon.\n",
            "Webpages and Scraping\n",
            "Web scraping is a method for extracting information from web pages. It enables RAG LLM models to access dynamic web content, making it a crucial source for real-time data retrieval.\n",
            "Databases and Structured Information\n",
            "Databases provide structured data that can be queried and extracted. Additionally, RAG models can utilize databases to retrieve specific information, enhancing their responses’ accuracy.\n",
            "Benefits of Retrieval-Augmented Generation (RAG)\n",
            "Let us now talk about the benefits of Retrieval Augmented Generation or RAG Model.\n",
            "Enhanced LLM Memory\n",
            "RAG addresses the information capacity limitation of traditional Language Models (LLMs). Traditional LLMs have a limited memory called “Parametric memory.” RAG introduces a “Non-Parametric memory” by tapping into external knowledge sources. This significantly expands the knowledge base of LLMs, enabling them to provide more comprehensive and accurate responses.\n",
            "Improved Contextualization\n",
            "RAG enhances the contextual understanding of LLMs by retrieving and integrating relevant contextual documents. This empowers the model to generate responses that align seamlessly with the specific context of the user’s input, resulting in accurate and contextually appropriate outputs.\n",
            "Updatable Memory\n",
            "A standout advantage of RAG is its ability to accommodate real-time updates and fresh sources without extensive model retraining. Moreover, this keeps the external knowledge base current and ensures that LLM-generated responses are always based on the latest and most relevant information.\n",
            "Source Citations\n",
            "RAG-equipped models can provide sources for their responses, thereby enhancing transparency and credibility. Moreover, users can access the sources that inform the LLM’s responses, promoting transparency and trust in AI-generated content.\n",
            "Reduced Hallucinations\n",
            "Studies have shown that RAG models exhibit fewer hallucinations and higher response accuracy. They are also less likely to leak sensitive information. Reduced hallucinations and increased accuracy make RAG models more reliable in generating content.\n",
            "These benefits collectively make Retrieval Augmented Generation (RAG) a transformative framework in Natural Language Processing. Consequently, it overcomes the limitations of traditional language models and enhances the capabilities of AI-powered applications.\n",
            "Diverse Approaches in RAG\n",
            "RAG Model offers a spectrum of approac\n",
            "\n",
            "    Please provide a response that incorporates information from these sources, and include citations in the format [Source X] where X is the source number.\n",
            "    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Response"
      ],
      "metadata": {
        "id": "-dypERakZJ2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nResponse:\")\n",
        "print(response)\n",
        "# display(Markdown(response))\n",
        "print(\"\\nSources:\")\n",
        "# display(Markdown(\"Sources:\"))\n",
        "for i, source in enumerate(sources, 1):\n",
        "    print(f\"[Source {i}] {source}\")\n",
        "    # display(Markdown(f\"[Source {i}] {source}\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ete-zUH5ZY3s",
        "outputId": "c03d4121-0a94-481a-8060-8d4dd7cb242f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Response:\n",
            "```python\n",
            "import streamlit as st\n",
            "import chromadb\n",
            "from chromadb.utils import embedding_functions\n",
            "import google.generativeai as genai\n",
            "import os\n",
            "from pypdf import PdfReader\n",
            "\n",
            "# --- Configure Gemini API (Ensure you have your API key set as GOOGLE_API_KEY environment variable) ---\n",
            "GOOGLE_API_KEY = os.environ.get(\"GOOGLE_API_KEY\")\n",
            "if not GOOGLE_API_KEY:\n",
            "    st.error(\"Please set your GOOGLE_API_KEY environment variable.\")\n",
            "    st.stop()\n",
            "genai.configure(api_key=GOOGLE_API_KEY)\n",
            "gemini_model = genai.GenerativeModel('gemini-1.5-flash-pro') # Using Gemini 1.5 Flash Pro model\n",
            "\n",
            "# --- Chroma Setup ---\n",
            "# Using in-memory Chroma for simplicity. For persistence, configure ChromaClient with a path.\n",
            "chroma_client = chromadb.Client()\n",
            "collection_name = \"rag_document_collection\"\n",
            "document_collection = chroma_client.get_or_create_collection(name=collection_name)\n",
            "\n",
            "# --- Embedding Function (Using Chroma's default, can be replaced with more specific ones if needed) ---\n",
            "# For more control, you could use sentence-transformers or OpenAI embeddings and initialize Chroma with that.\n",
            "# Example using sentence-transformers (you'd need to install sentence-transformers):\n",
            "# chroma_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-mpnet-base-v2\")\n",
            "# document_collection = chroma_client.get_or_create_collection(name=collection_name, embedding_function=chroma_ef)\n",
            "\n",
            "\n",
            "# --- Function to Extract Text from PDF ---\n",
            "def extract_text_from_pdf(pdf_file):\n",
            "    \"\"\"\n",
            "    Extracts text content from a PDF file.\n",
            "\n",
            "    Args:\n",
            "        pdf_file: UploadedFile object from Streamlit or path to a PDF file.\n",
            "\n",
            "    Returns:\n",
            "        str: Extracted text content from the PDF.\n",
            "             Returns None if there's an error during extraction.\n",
            "    \"\"\"\n",
            "    text = \"\"\n",
            "    try:\n",
            "        pdf_reader = PdfReader(pdf_file)\n",
            "        for page in pdf_reader.pages:\n",
            "            text += page.extract_text()\n",
            "        return text\n",
            "    except Exception as e:\n",
            "        st.error(f\"Error extracting text from PDF: {e}\")\n",
            "        return None\n",
            "\n",
            "# --- Function to Embed and Store Documents in Chroma ---\n",
            "def embed_and_store_documents(pdf_files):\n",
            "    \"\"\"\n",
            "    Embeds text from PDF files and stores them in Chroma.\n",
            "\n",
            "    Args:\n",
            "        pdf_files: List of UploadedFile objects from Streamlit.\n",
            "\n",
            "    Returns:\n",
            "        bool: True if embedding and storing are successful, False otherwise.\n",
            "    \"\"\"\n",
            "    all_texts = []\n",
            "    all_metadatas = []\n",
            "    all_ids = []\n",
            "\n",
            "    for pdf_file in pdf_files:\n",
            "        text_content = extract_text_from_pdf(pdf_file)\n",
            "        if text_content:\n",
            "            all_texts.append(text_content)\n",
            "            all_metadatas.append({\"source\": pdf_file.name}) # Metadata to track source\n",
            "            all_ids.append(pdf_file.name) # Using filename as ID, ensure unique filenames\n",
            "\n",
            "    if all_texts:\n",
            "        try:\n",
            "            document_collection.add(\n",
            "                embeddings=None, # Chroma will handle embeddings if embedding_function is set during collection creation or using default.\n",
            "                documents=all_texts,\n",
            "                metadatas=all_metadatas,\n",
            "                ids=all_ids\n",
            "            )\n",
            "            st.success(f\"Successfully embedded and stored {len(all_texts)} documents in Chroma.\")\n",
            "            return True\n",
            "        except Exception as e:\n",
            "            st.error(f\"Error embedding and storing documents in Chroma: {e}\")\n",
            "            return False\n",
            "    else:\n",
            "        st.warning(\"No text extracted from uploaded PDFs to store.\")\n",
            "        return False\n",
            "\n",
            "\n",
            "# --- Function to Query Chroma for Relevant Documents ---\n",
            "def query_chroma(query, k=3):\n",
            "    \"\"\"\n",
            "    Queries Chroma for the top-k most relevant documents for a given query.\n",
            "\n",
            "    Args:\n",
            "        query (str): User query.\n",
            "        k (int): Number of top documents to retrieve.\n",
            "\n",
            "    Returns:\n",
            "        list: List of retrieved documents (dictionaries) from Chroma, or None if no results.\n",
            "    \"\"\"\n",
            "    try:\n",
            "        results = document_collection.query(\n",
            "            query_texts=[query],\n",
            "            n_results=k\n",
            "        )\n",
            "        if results and results['documents'] and results['documents'][0]: # Check if documents are retrieved\n",
            "            retrieved_docs = []\n",
            "            for i in range(len(results['documents'][0])): # Iterate through retrieved documents\n",
            "                doc_content = results['documents'][0][i]\n",
            "                doc_metadata = results['metadatas'][0][i]\n",
            "                retrieved_docs.append({\"content\": doc_content, \"metadata\": doc_metadata})\n",
            "            return retrieved_docs\n",
            "        else:\n",
            "            st.warning(\"No relevant documents found in the database for the query.\")\n",
            "            return None\n",
            "    except Exception as e:\n",
            "        st.error(f\"Error querying Chroma: {e}\")\n",
            "        return None\n",
            "\n",
            "\n",
            "# --- Function to Generate Response using Google Gemini 1.5 Flash ---\n",
            "def generate_response_gemini(query, context_documents):\n",
            "    \"\"\"\n",
            "    Generates a response using Google Gemini 1.5 Flash, augmented with context from retrieved documents.\n",
            "\n",
            "    Args:\n",
            "        query (str): User query.\n",
            "        context_documents (list): List of retrieved documents (dictionaries).\n",
            "\n",
            "    Returns:\n",
            "        str: Generated response from Gemini, or None if there's an error.\n",
            "    \"\"\"\n",
            "    if not context_documents:\n",
            "        return \"No relevant documents found to provide context. Please try a different query or upload relevant documents.\"\n",
            "\n",
            "    context_text = \"\\n\\n\".join([doc[\"content\"] for doc in context_documents]) # Concatenate document contents\n",
            "    prompt_content = f\"\"\"Answer the following question based on the context provided.\n",
            "    Context:\n",
            "    {context_text}\n",
            "\n",
            "    Question: {query}\n",
            "    \"\"\"\n",
            "\n",
            "    try:\n",
            "        response = gemini_model.generate_content(prompt_content)\n",
            "        return response.text\n",
            "    except Exception as e:\n",
            "        st.error(f\"Error generating response from Gemini: {e}\")\n",
            "        return None\n",
            "\n",
            "\n",
            "# --- Streamlit App ---\n",
            "def streamlit_app():\n",
            "    st.title(\"RAG System with Gemini 1.5 Flash, Chroma, and Streamlit\")\n",
            "\n",
            "    # --- Sidebar for Document Upload ---\n",
            "    with st.sidebar:\n",
            "        st.header(\"Document Upload\")\n",
            "        uploaded_files = st.file_uploader(\"Upload PDF documents\", type=[\"pdf\"], accept_multiple_files=True)\n",
            "        if uploaded_files:\n",
            "            if st.sidebar.button(\"Process and Store Documents\"):\n",
            "                with st.spinner(\"Processing and storing documents...\"):\n",
            "                    embed_and_store_documents(uploaded_files)\n",
            "\n",
            "    # --- Main Panel for Query and Response ---\n",
            "    query_text = st.text_input(\"Enter your query:\", placeholder=\"Ask me anything about the uploaded documents\")\n",
            "\n",
            "    if query_text:\n",
            "        if st.button(\"Get Response\"):\n",
            "            with st.spinner(\"Retrieving documents and generating response...\"):\n",
            "                retrieved_documents = query_chroma(query_text)\n",
            "\n",
            "                st.subheader(\"Retrieved Documents:\")\n",
            "                if retrieved_documents:\n",
            "                    for doc in retrieved_documents:\n",
            "                        st.markdown(f\"**Source:** {doc['metadata']['source']}\")\n",
            "                        st.write(doc['content'][:300] + \"...\") # Display snippet of document\n",
            "                        st.expander(\"See full document\").write(doc['content']) # Expand to see full doc\n",
            "                        st.divider()\n",
            "                else:\n",
            "                    st.info(\"No relevant documents retrieved for this query.\")\n",
            "\n",
            "                generated_response = generate_response_gemini(query_text, retrieved_documents)\n",
            "\n",
            "                st.subheader(\"Generated Response:\")\n",
            "                if generated_response:\n",
            "                    st.write(generated_response)\n",
            "                else:\n",
            "                    st.error(\"Failed to generate response.\")\n",
            "\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    streamlit_app()\n",
            "```\n",
            "\n",
            "**Explanation:**\n",
            "\n",
            "1.  **Import Libraries:** Imports necessary libraries like `streamlit`, `chromadb`, `google.generativeai`, `os`, and `pypdf`.\n",
            "2.  **Configure Gemini API:**\n",
            "    *   Retrieves the Google API key from the environment variable `GOOGLE_API_KEY`. **Important:** You need to set this environment variable with your actual Gemini API key.\n",
            "    *   Configures the `genai` library with the API key.\n",
            "    *   Initializes the `gemini-1.5-flash-pro` model.\n",
            "3.  **Chroma Setup:**\n",
            "    *   Creates a Chroma client (in-memory for this example). For persistent storage, you would configure the client with a path.\n",
            "    *   Creates or retrieves a Chroma collection named \"rag\\_document\\_collection\".\n",
            "    *   Uses Chroma's default embedding function. You can customize this by specifying an `embedding_function` during collection creation (e.g., using `sentence-transformers`).\n",
            "4.  **`extract_text_from_pdf(pdf_file)` Function:**\n",
            "    *   Takes a PDF file (Streamlit `UploadedFile` object) as input.\n",
            "    *   Uses `pypdf` to read the PDF and extract text from each page.\n",
            "    *   Includes error handling for PDF reading.\n",
            "5.  **`embed_and_store_documents(pdf_files)` Function:**\n",
            "    *   Takes a list of PDF files as input.\n",
            "    *   Iterates through each PDF, extracts text using `extract_text_from_pdf`.\n",
            "    *   Prepares lists for texts, metadata (source filename), and IDs (filenames).\n",
            "    *   Uses `document_collection.add()` to add the documents to Chroma. Chroma handles embedding automatically if an `embedding_function` was configured or defaults are used.\n",
            "    *   Includes error handling for Chroma operations.\n",
            "6.  **`query_chroma(query, k=3)` Function:**\n",
            "    *   Takes a user query and `k` (number of results) as input.\n",
            "    *   Uses `document_collection.query()` to search Chroma for documents relevant to the query.\n",
            "    *   Retrieves top-k documents.\n",
            "    *   Includes error handling and checks for no retrieved documents.\n",
            "7.  **`generate_response_gemini(query, context_documents)` Function:**\n",
            "    *   Takes a user query and a list of retrieved documents as context.\n",
            "    *   Handles the case where no context documents are provided.\n",
            "    *   Concatenates the content of retrieved documents into a context string.\n",
            "    *   Constructs a prompt that includes the context and the user's question.\n",
            "    *   Uses `gemini_model.generate_content()` to get a response from Gemini.\n",
            "    *   Includes error handling for Gemini API calls.\n",
            "8.  **`streamlit_app()` Function:**\n",
            "    *   Sets up the Streamlit user interface.\n",
            "    *   **Sidebar (Document Upload):**\n",
            "        *   Uses `st.sidebar` to create a sidebar for document upload.\n",
            "        *   `st.file_uploader` allows users to upload PDF files.\n",
            "        *   A \"Process and Store Documents\" button triggers the `embed_and_store_documents` function.\n",
            "    *   **Main Panel (Query and Response):**\n",
            "        *   `st.text_input` for users to enter their query.\n",
            "        *   A \"Get Response\" button triggers the RAG workflow.\n",
            "        *   **Document Retrieval and Display:** Calls `query_chroma` to retrieve documents. Displays retrieved documents with source filenames and snippets using `st.markdown`, `st.write`, and `st.expander`.\n",
            "        *   **Response Generation and Display:** Calls `generate_response_gemini` to get a response from Gemini. Displays the generated response using `st.write`.\n",
            "9.  **`if __name__ == \"__main__\":` block:**\n",
            "    *   Ensures that the `streamlit_app()` function is called when the script is run directly.\n",
            "\n",
            "**To run this script:**\n",
            "\n",
            "1.  **Install Libraries:**\n",
            "    ```bash\n",
            "    pip install streamlit chromadb google-generativeai pypdf\n",
            "    ```\n",
            "2.  **Set API Key:**  Set your Google Gemini API key as an environment variable named `GOOGLE_API_KEY`.\n",
            "3.  **Run Streamlit App:**\n",
            "    ```bash\n",
            "    streamlit run your_script_name.py\n",
            "    ```\n",
            "    Replace `your_script_name.py` with the name of your Python file.\n",
            "\n",
            "**Key improvements and considerations:**\n",
            "\n",
            "*   **Modularity:** The code is well-structured with functions for each major step (extraction, embedding, querying, generation, UI).\n",
            "*   **Error Handling:** Includes `try...except` blocks for PDF extraction, Chroma operations, and Gemini API calls, providing informative error messages in the Streamlit UI.\n",
            "*   **Comments:**  Code is commented to explain the purpose of functions and key steps.\n",
            "*   **Streamlit UI:** Provides a basic but functional UI with file upload, query input, and display of retrieved documents and responses.\n",
            "*   **Dynamic PDF Processing:** Uploaded PDFs are processed and added to Chroma without requiring a server restart.\n",
            "*   **Source Citations (in UI):**  Displays the source filename for each retrieved document.\n",
            "*   **Snippet and Expander for Documents:** Shows a snippet of retrieved documents and provides an expander to view the full content, improving UI readability.\n",
            "*   **No Relevant Documents Handling:**  Handles cases where no relevant documents are found and informs the user.\n",
            "*   **Assumptions:**  Relies on the assumption that the Google Gemini API key is correctly set and that the necessary libraries are installed.  You might want to add explicit checks for library installations and guide the user.\n",
            "*   **Embedding Model:**  Uses Chroma's default embedding. For production systems, consider using a more specific and robust embedding model (e.g., from sentence-transformers or OpenAI) and explicitly configuring Chroma with that model.\n",
            "*   **Prompt Engineering:** The prompt is basic. For better Gemini performance, more sophisticated prompt engineering might be needed depending on the nature of your documents and queries.\n",
            "*   **Scalability and Persistence:** In-memory Chroma is suitable for demos. For real-world applications, use a persistent Chroma setup and consider optimizations for handling large document collections.\n",
            "\n",
            "Sources:\n",
            "[Source 1] https://python.langchain.com/docs/tutorials/rag/\n",
            "[Source 2] https://joelotepawembo.medium.com/how-to-create-a-retrieval-augmented-generation-rag-system-and-pipeline-using-deepseek-langchain-741595cfda32\n",
            "[Source 3] https://www.youtube.com/watch?v=NnXHVOQJbQ4\n",
            "[Source 4] https://www.reddit.com/r/Python/comments/1dh3jtj/built_a_rag_retrievalaugmented_generation_model/\n",
            "[Source 5] https://www.analyticsvidhya.com/blog/2023/09/retrieval-augmented-generation-rag-in-ai/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kTYpvehweuxh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOF7BOwjkHkSUKoEtmLD94r",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}